{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a from scratch implementation of a simple RNN having 2 hidden layers\n",
    "# it reads in any text file, tries it to learn character by character and then generates sentences\n",
    "# the basic code is taken from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "data = open('BThesis.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "totalCharacters = len(data)\n",
    "vocabLen = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'>': 0, 'f': 1, 'S': 2, 'd': 3, '”': 4, 'r': 5, 'R': 6, 'x': 7, '+': 8, '-': 9, '–': 10, 'l': 11, 'T': 12, '[': 13, 'F': 14, 's': 15, 'N': 16, 'O': 17, 'W': 18, 'z': 19, 'q': 20, 'I': 21, 'o': 22, '—': 23, 'P': 24, 'n': 25, 'k': 26, 'K': 27, '<': 28, 'a': 29, ' ': 30, ']': 31, 'h': 32, 'y': 33, 'i': 34, 'C': 35, 'w': 36, 'X': 37, 'u': 38, ':': 39, ',': 40, '/': 41, 'D': 42, 'm': 43, '’': 44, 'M': 45, '\\n': 46, 'Y': 47, 'g': 48, 'Q': 49, '=': 50, 'e': 51, 'j': 52, 'c': 53, '_': 54, '“': 55, 'v': 56, 'A': 57, '.': 58, 'U': 59, 'p': 60, 'b': 61, 't': 62, 'G': 63, 'V': 64, 'B': 65, 'L': 66, 'E': 67, 'H': 68}\n",
      "{0: '>', 1: 'f', 2: 'S', 3: 'd', 4: '”', 5: 'r', 6: 'R', 7: 'x', 8: '+', 9: '-', 10: '–', 11: 'l', 12: 'T', 13: '[', 14: 'F', 15: 's', 16: 'N', 17: 'O', 18: 'W', 19: 'z', 20: 'q', 21: 'I', 22: 'o', 23: '—', 24: 'P', 25: 'n', 26: 'k', 27: 'K', 28: '<', 29: 'a', 30: ' ', 31: ']', 32: 'h', 33: 'y', 34: 'i', 35: 'C', 36: 'w', 37: 'X', 38: 'u', 39: ':', 40: ',', 41: '/', 42: 'D', 43: 'm', 44: '’', 45: 'M', 46: '\\n', 47: 'Y', 48: 'g', 49: 'Q', 50: '=', 51: 'e', 52: 'j', 53: 'c', 54: '_', 55: '“', 56: 'v', 57: 'A', 58: '.', 59: 'U', 60: 'p', 61: 'b', 62: 't', 63: 'G', 64: 'V', 65: 'B', 66: 'L', 67: 'E', 68: 'H'}\n"
     ]
    }
   ],
   "source": [
    "characterToIndex = {ch:i for i,ch in enumerate(chars)}\n",
    "print(characterToIndex)\n",
    "indexToCharacter = {i:ch for i,ch in enumerate(chars)}\n",
    "print(indexToCharacter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#hyperparameters\n",
    "learningRate = 0.01\n",
    "hiddenLayer = 256\n",
    "seqLength = 50\n",
    "#modelParameters\n",
    "#connect input layer to first hidden layer\n",
    "W1 = np.random.randn(vocabLen, hiddenLayer) * 0.01\n",
    "# connect second hidden layer to output layer\n",
    "W2 = np.random.randn(hiddenLayer, vocabLen) * 0.01\n",
    "# connect first hidden layer to first hidden layer in the next timestamp\n",
    "Wh1 = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# connect the first hidden layer to the second hidden layer\n",
    "Whh = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# connect the second hidden layer to the second hidden layer in the next timestamp\n",
    "Wh2 = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# bias for W1\n",
    "b1 = np.random.randn(hiddenLayer, 1)\n",
    "# bias for Whh\n",
    "bh = np.random.randn(hiddenLayer, 1)\n",
    "# bias for W2\n",
    "b2 = np.random.randn(vocabLen, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the loss function would take in the input chars, the output chars and the previous hidden state\n",
    "# it outputs the hidden state, the gradients for each parameter between layers and the last hidden states\n",
    "def propagate(inputChars, outputChars, prevH, prevH1):\n",
    "    x, h, h1, y, p = {}, {}, {}, {}, {}\n",
    "    #x = the array which is a list of zeros, with just 1 at the index where input character is\n",
    "    #h = values of hidden layers at different times\n",
    "    #y = values of outputs not activated\n",
    "    #p = activated output\n",
    "    h[-1] = np.copy(prevH)\n",
    "    h1[-1] = np.copy(prevH1)\n",
    "    loss = 0\n",
    "    #forward propagation\n",
    "    for t in range(len(inputChars)):\n",
    "        x[t] = np.zeros((vocabLen, 1))\n",
    "        x[t][inputChars[t]] = 1\n",
    "        h[t] = np.tanh(W1.T.dot(x[t]) + Wh1.dot(h[t-1]) + b1) # h has to be of the same dimension as b\n",
    "        h1[t] = np.tanh(Whh.T.dot(h[t]) + Wh2.dot(h1[t-1]) + bh)\n",
    "        y[t] = W2.T.dot(h1[t]) + b2\n",
    "        p[t] = np.exp(y[t])/np.sum(np.exp(y[t]))\n",
    "        loss += -np.log(p[t][outputChars[t],0])\n",
    "    \n",
    "    \n",
    "    #backward propagation\n",
    "    dW1, dW2, dWh1, dWhh, dWh2 = np.zeros_like(W1), np.zeros_like(W2), np.zeros_like(Wh1), np.zeros_like(Whh), np.zeros_like(Wh2)\n",
    "    db1, db2, dbh = np.zeros_like(b1), np.zeros_like(b2), np.zeros_like(bh)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    dh1next = np.zeros_like(h1[0])\n",
    "    for t in reversed(range(len(inputChars))):\n",
    "        dy = np.copy(p[t])\n",
    "        #starting the backpropagation\n",
    "        dy[outputChars[t]] -= 1\n",
    "        \n",
    "        dW2 += h1[t].dot(dy.T)\n",
    "        db2 += dy\n",
    "        #generally the error is backpropageted by multiplying the error in the output to the input's transpose\n",
    "        #here the transport is of the error, but this is because the way i have initialized the shape of the matrices\n",
    "        #in the original implementation, it is the standard way\n",
    "        \n",
    "        #second hidden layer backpropagation\n",
    "        dh1 = np.dot(W2, dy) + dh1next\n",
    "        #we got the below line of code by differentiation of the activation function (tanh)\n",
    "        dh1raw = (1 - h1[t] * h1[t]) * dh1\n",
    "        # so basically the activation is differentiated in two steps\n",
    "        # first is the values inside the tanh function (dh1), then finally the tanh function itself,\n",
    "        # and as from the chain rule dh1 is multiplied to it\n",
    "        dbh += dh1raw\n",
    "        dWhh += np.dot(h[t], dh1raw.T)\n",
    "        dWh2 += np.dot(dh1raw, h1[t-1].T)\n",
    "        \n",
    "        #first hidden layer backpropagation\n",
    "        dh = np.dot(Whh, dh1) + dh1next\n",
    "        dhraw = (1 - h[t] * h[t]) * dh\n",
    "        db1 += dhraw\n",
    "        dW1 += np.dot(x[t], dhraw.T) #derivative of input to hidden layer weight\n",
    "        dWh1 += np.dot(dhraw, h[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Wh1.T, dhraw)\n",
    "    for dparam in [dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2, h[len(inputChars)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " te recommender system: as trid s thete, incoud skere information lion of mimit d, to to matrix when we added to crevic be of tambed Sendiecality have that I had thic lould be user nationsed us to sear \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, h1, seed, n):\n",
    "                                                                                                                                                                                        \n",
    "    #sample a sequence of integers from the model                                                                                                                                                \n",
    "    #h is memory state, seed is seed letter for first time step   \n",
    "    #n is how many characters to predict\n",
    "\n",
    "    x = np.zeros((vocabLen, 1))\n",
    "    x[seed] = 1\n",
    "    #list to store generated chars\n",
    "    outputChars = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(W1.T, x) + np.dot(Wh1, h) + b1)\n",
    "        h1 = np.tanh(np.dot(Whh.T, h) + np.dot(Wh2, h1) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(W2.T, h1) + b2\n",
    "        # probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #print(p)\n",
    "        #pick one with the highest probability \n",
    "        selectedChar = np.random.choice(range(vocabLen), p=p.ravel())\n",
    "        #print(ix)\n",
    "        #create a vector\n",
    "        x = np.zeros((vocabLen, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[selectedChar] = 1\n",
    "        #add it to the list\n",
    "        outputChars.append(selectedChar)\n",
    "\n",
    "    txt = ''.join(indexToCharacter[char] for char in outputChars)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "    hprev = np.zeros((hiddenLayer,1)) # reset RNN memory  \n",
    "    h1prev = np.zeros((hiddenLayer,1)) # reset RNN memory \n",
    "    #predict the 200 next characters given 'a'\n",
    "sample(hprev,h1prev,characterToIndex['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 211.514828\n",
      "----\n",
      "  usuge difees as to n is that to suedses way morefstived as additional to learn, the apto pred.\n",
      "\n",
      "Adir, Filtering. The mading ablitid sparse data. Buth acherefore we need. In to a form ardedict in of t \n",
      "----\n",
      "iter 1000, loss: 128.568182\n",
      "----\n",
      " nfror of sto sest Thith bervioul information and co. Loviouct in towe . ]G fove sigris an ompes tive sems us tu omore seormse woice of a so move gilter trix of t ives coul best in the in a movien this \n",
      "----\n",
      "iter 2000, loss: 83.097862\n",
      "----\n",
      " a fried if a gurt . when we addes or folthes wate  osed eatece to eecels and clesention, ar ititially. The fir allystesprally go calculatent imally enterselt. We woweldasus alrowithrs. The ghow whe th \n",
      "----\n",
      "iter 3000, loss: 62.130336\n",
      "----\n",
      " tion hechncensty. There gromptilit is to theithm to add to M tenser mode approady the user, Pequstict of thowherged . This ourgow ition torm approach boove watrix factorization methochibes  ore berm i \n",
      "----\n",
      "iter 4000, loss: 52.206116\n",
      "----\n",
      "  goo teed of by-endef of the metadata was rescther of algroach That is that I havel thewbine gegrods the kecompwerion rerses it  sect cored cluce a go teon in tacceste have been a term: it kot vequace \n",
      "----\n",
      "iter 5000, loss: 46.906635\n",
      "----\n",
      " ran we bot is files in the simies on methods twe probuct is mavy e tushor lied extra sef re followhich inpllece . Alfogivem conne have tr- lich was Resed proplems and me prediares onprose spart. Mas   \n",
      "----\n",
      "iter 6000, loss: 44.087787\n",
      "----\n",
      " y for dive this matred Seth se are generalute the nat iftermor wor randreary to the gesteringin to all the factorization machine in the advaris the sifferent apperoverented which used by Ond save beer \n",
      "----\n",
      "iter 7000, loss: 41.783908\n",
      "----\n",
      " erefore and also FFdelf approach Thiffen muchind and nlalaty as has vatFimen in neconde, sls twe reasing almod MF matrix. Ton bed ca use the processed offectod entwied and grocle approachs Neel sears  \n",
      "----\n",
      "iter 8000, loss: 40.655376\n",
      "----\n",
      " ucs the movies. The figire or vied a d s ateat wealplest metadata. It is the similarity Masp of thich fiD nded to purmilured thenenaty as cans alforithms with fased of then tarearse fore wecenss lomse \n",
      "----\n",
      "iter 9000, loss: 39.110025\n",
      "----\n",
      " fondent metadata en lecala. The timmined ally reductin-CG use , bata red tact.  CD appprough the user about fork the inprovested than matreconn ly Neco usimination in itsuefcrovements har users are we \n",
      "----\n",
      "iter 10000, loss: 38.544684\n",
      "----\n",
      " ser factorization machines that we nee has in s nur to moritadill wilurixe perkidem enes ta find limit datanot peardation was mul them that is the ncan as ty see trom amet.\n",
      " Ne all  ateor-with dest th \n",
      "----\n",
      "iter 11000, loss: 37.359340\n",
      "----\n",
      " plry of in, bate. We how  panne is allofvery serys tad inprappomiss. Figstes a uey in problewe tiltaboon Inorecosed eives and als those of this approach. I have sted model Bases inclearnsilixifizita o \n",
      "----\n",
      "iter 12000, loss: 37.081499\n",
      "----\n",
      " escluster by addprated and Vhypers lelention. The SVMs that tupes it the SVMs with the provioueses has ding is the rifferent approach oud the flustering. In  mes mest very the absilize tensorintersed, \n",
      "----\n",
      "iter 13000, loss: 35.923275\n",
      "----\n",
      " udding oven the rmse values adding chass apees to sen the results bike stayse callud the serch this apors hal diert spprest these pro ping users. We weny Aplange is simoly, this method of the systems  \n",
      "----\n",
      "iter 14000, loss: 35.823252\n",
      "----\n",
      " aby dovies  . Sothodest pased a senequadatata alged the actors, producevsing terdering liker. The follicialy in the data in this paspor contimesting the appovies. For the appoidice. In  . The fealor s \n",
      "----\n",
      "iter 15000, loss: 34.712953\n",
      "----\n",
      " bacltowns actey vish broved to the mome seoun informative Filter acon the basing and then the west bise  ling list a pofniexper inderence becaus some omporate, Fimse besulate a fifms indu tovization f \n",
      "----\n",
      "iter 16000, loss: 34.807404\n",
      "----\n",
      " ilispre Soveres can be joust s items. It is to the factors. The byst metadatagen we and the vabrivending algorization machinesizeyicompropures. There are perspect algo there are vergen by the user aco \n",
      "----\n",
      "iter 17000, loss: 33.675842\n",
      "----\n",
      " metial the factorization was bibel, SVMs. FM, iss afforickm, matrix MG includes in a ddem th ilstra Grid them is factine . The thin the upprouk the I Approach shing the average rasistestimilation MF.  \n",
      "----\n",
      "iter 18000, loss: 33.754523\n",
      "----\n",
      "  mopa lurs are on the first I couldyliger. The majondd wivels like to norization. The recommand t beris very likn mundimprepres that I were o papticular than tatadd conting Far poris. If the user. In  \n",
      "----\n",
      "iter 19000, loss: 32.837726\n",
      "----\n",
      " nchis for this timmen that the user appling has neencthery is cand al oroviyiSe a proal [ it the cast is the samen them the matrix factorization mithice random shar matrix mactorizaniry, it predient b \n",
      "----\n",
      "iter 20000, loss: 32.808112\n",
      "----\n",
      " pothes. It habre choly maneat ticles, resunteast sue predictilate is acon be imersias as dalle similarity wimicar vence is nte thancentey how matree I was where are gett it gur the spares that ollyin  \n",
      "----\n",
      "iter 21000, loss: 32.129166\n",
      "----\n",
      " rsex. In this  auntersum is nod the users. There is a motrodplrow metadata hiver genre hanue of the vars d itaroals in telll woratine ratrix factorization and factorization to Sims. The bior intormati \n",
      "----\n",
      "iter 22000, loss: 31.998738\n",
      "----\n",
      " l vilust thevent Facter sod each Qions are Reta. In  lgy, of Sactorequix a lith, of the form of information in muct, shet this approach in the informatio handle spllor give in talchod wially eg. I tri \n",
      "----\n",
      "iter 23000, loss: 31.423040\n",
      "----\n",
      " rid Approach wasings \n",
      ", rexels, Yodptuad inte beltion by watchat the recome times wovinitifigune which t and bers as in this. This fierxin losk reporiestrag the predient reaginf cears to mimially co m \n",
      "----\n",
      "iter 24000, loss: 31.206566\n",
      "----\n",
      " imilarity matrix Us wolls similarity in ter erare groidiDerrary wasedipata a thecemser munctred. Ksers of fooumided aboration. The finur dergen and for the moderecorserally getanataseatwerar in oucron \n",
      "----\n",
      "iter 25000, loss: 30.716785\n",
      "----\n",
      " enen a mod to udems are of the information froblemboution timpllarn, whele information clused of the pertent metadata in ormatica. The gesearther, inse pore weren in hyscensprousunt metadata hirs tena \n",
      "----\n",
      "iter 26000, loss: 30.513522\n",
      "----\n",
      " ers mecape puted of imsttear that I cas algorithm of intere a thrinks way better the centenestuppriady I the inturvery varitF, etupplicate is is that ix kest ty approach, FMs. Fore che simer in Sector \n",
      "----\n",
      "iter 27000, loss: 30.201742\n",
      "----\n",
      " n additional information, the thich for conle to Betarata ted mork I lixt Peatsed efficht wooks of e form sem of sametors have been usimilarly, mut ix ravis of rmstra whow the-ratrix UG waller oun rea \n",
      "----\n",
      "iter 28000, loss: 29.973674\n",
      "----\n",
      " nes the Whrouged it isselisionte, to its uas a wase of maty of tumamen at this sas gfouthes the improvement is peo lerenstimes pred. This case taskine sizar bas tata. The information in steporibute be \n",
      "----\n",
      "iter 29000, loss: 29.662717\n",
      "----\n",
      " dy tom. SVMs, FMs are sticknor alabization wasenit metamata lasion that MC Chen It byilg to le nath and  pin-alation ond peling in nuctorization machines r-acan Fiffey which candon, eacandecrations it \n",
      "----\n",
      "iter 30000, loss: 29.428171\n",
      "----\n",
      " sid  inture hafied SVMs, follebser a for than a govioks redicate, etcend vs Cowe. The mode of mpskire ory merkor the move abstrated in the isers based M-pever, to dictlla hicterand that of mmares. The \n",
      "----\n",
      "iter 31000, loss: 29.205407\n",
      "----\n",
      "  the one noke weror  onk in te memensted to convious ffiend wo like sum wateg.   moues we reword not whrough which factor angroach apped tho predicted is the basic mmen the fifferent ind for the matri \n",
      "----\n",
      "iter 32000, loss: 28.907414\n",
      "----\n",
      "  be aust additioalbol. Nor the nome als. Soonsers all  models. ethel.  forofules me assulate a sighlas to calculate in gervan the metadata as it a groach Thin in genresearch has proficwor, incour a Us \n",
      "----\n",
      "iter 33000, loss: 28.771415\n",
      "----\n",
      " sed the rate. When we anded thete matrid mach row and aperovimely, add cles we adatagine approacher. The matrix fod is the factorization and far lof the ficht dion fornt.  ftermaris factorizatiog Man  \n",
      "----\n",
      "iter 34000, loss: 28.383398\n",
      "----\n",
      " is, atates not applied dopter usinn ory fest metadata, gecomeducebofis norfory of tya cinteratems arse data. In  eans a cucontent basis tas the alre some able to ard chesentine at asticks for approach \n",
      "----\n",
      "iter 35000, loss: 28.361782\n",
      "----\n",
      " uc ited the preperidem when by jup to the wablit factorization with casteasproprid swacimpochive to nowilar that  G similatit my approaches to alltra chated pprove serves onl frodies feant giginralles \n",
      "----\n",
      "iter 36000, loss: 27.953030\n",
      "----\n",
      " fondiced intere ulgem a cimomene and then sork. This is calchick has net Fig mike tenlen more the ine the folloractiarly thece ore abstrated in and ugen is cante wenre almosted to finure out the movie \n",
      "----\n",
      "iter 37000, loss: 28.004977\n",
      "----\n",
      " a averse . As the neablicie. MA tind about the this matrix that teplod factorization model, eccuise shows mor of ] prochasturest meterith ondex, rech de aucrofeng tomarvs earchiod  sulo information wi \n",
      "----\n",
      "iter 38000, loss: 27.634770\n",
      "----\n",
      " plich hat in a dveraly preduce the similarity as Ralged ALSerer aborearely Sos aldisia ly rquind werk a pastin-by SGD abpuctilike relaveding cound betueat the best resuendeter eval in. Alsorithm wo le \n",
      "----\n",
      "iter 39000, loss: 27.753153\n",
      "----\n",
      " , maries. They wo is very larmable data whin te which it the s, this as would to improve this matrix is verks st colloincteat timinding the metadata hy the information from that can be using intt in p \n",
      "----\n",
      "iter 40000, loss: 27.286674\n",
      "----\n",
      " nge which the similary The infostable shis matres factorization maching Far about suedfoctorid abolltion wo the startawnowsing nofy alsoration Macline. These providers. They the user-matrie”. Cl sredi \n",
      "----\n",
      "iter 41000, loss: 27.471790\n",
      "----\n",
      " oun thesefficitially when muthod imploous the best osed that we did trimasiminicine imimikes us watinfm this sectors search for a maveirecaix is tover al t od eve systeadivenss cellid the getcomptrede \n",
      "----\n",
      "iter 42000, loss: 26.920179\n",
      "----\n",
      " ative Filtering Gron approach is to ivs, it is aur of inter in lig[ inspritems an the binary value elser orx frich lide “casenory likento iefof movie, we k-st As would bent worveduce the kerun the rea \n",
      "----\n",
      "iter 43000, loss: 27.210092\n",
      "----\n",
      " al ever and the abstractosizit domengens s, Parmor applith some tuces.  Constrien as jued us vary wartthype hastors have A fiminen . The tasa they are hing is, masicted a west metadata-. The informati \n",
      "----\n",
      "iter 44000, loss: 26.589846\n",
      "----\n",
      " chas sien den wasched cendle thaining sectived to the itleont dimentean then they. Buratly Whey we impretases Usistensifformst poustridion and this genreted of the rate is conve. to matchit as through \n",
      "----\n",
      "iter 45000, loss: 26.820843\n",
      "----\n",
      "  than mithin soke content metadata to hanue of the ferther nactors mearnon for con thark to man molt of prid sparse acsivioitime aspararelen in there have veen re sernft.  ,  Werization machines has l \n",
      "----\n",
      "iter 46000, loss: 26.292090\n",
      "----\n",
      " cems given by the usershic to sanclion contaborative filtering ath al the rmpetare test as a glalize thre by leed that a usereformalading search now expuser, it andlying to secon- aracliarar this in t \n",
      "----\n",
      "iter 47000, loss: 26.428533\n",
      "----\n",
      " indita is impleme hidective is stent metadata jot of ermes to predict the movies that in a bypor hander systems ratt ins Hy opalsiof time and anoun by fupessik to have recommender systems than s, wist \n",
      "----\n",
      "iter 48000, loss: 26.024847\n",
      "----\n",
      " rks the wais. it yid nels of the and beher metrad and then are sing waidsel. The user in macred the product, is to there weraper  Grom the information un increase tried to incled ceeved jothy. The rea \n",
      "----\n",
      "iter 49000, loss: 26.057747\n",
      "----\n",
      " ces us where metadata Thich of collaractext applier conted to lince incouctors, prodeny Dimilarly felch por productrer even in preprased of tomenteen effo spoct: sestememope better than tems. Nethodel \n",
      "----\n",
      "iter 50000, loss: 25.712837\n",
      "----\n",
      " rid matrix thow hiswer by faccuithm. Outhome somefines over the retadata thate inntood ertwerent them to lyper-parasettris out stering and blivie there the apporated in the results we bot in is that f \n",
      "----\n",
      "iter 51000, loss: 25.679077\n",
      "----\n",
      " imicar. The compute this ifformation. The preferont cased computten the hynes. Sogrosensing sas visural repodiegretsering ore is for different il taskes ar wease that we is de grkity in implemenation  \n",
      "----\n",
      "iter 52000, loss: 25.407323\n",
      "----\n",
      " er. st of hade the metadata into catent factorization machines well. This var the results user th have digsicor the orine there it  sea that are these hiddingresser. Lecommendation is that the user mi \n",
      "----\n",
      "iter 53000, loss: 25.321396\n",
      "----\n",
      " as werengle the werdictich tells oue is recommender systems not on the figule milchid ceat than that int emparcating: rmpraprese sint ow sime Fapriceceried ty there werouged its. et wenset the etring  \n",
      "----\n",
      "iter 54000, loss: 25.198776\n",
      "----\n",
      "  fistoringnormaching the wettore dack of ext intentiond sying the racorng: we dow tives to gose raced in the past metablica contres wickine ate the user. Por appleage The dimension was not lifforms fo \n",
      "----\n",
      "iter 55000, loss: 25.089371\n",
      "----\n",
      " nsinter. I ticularial to be sigit phas works int of the challenge of the matrix factorization as it is is calcinhormata. but on the movies and aboriest I te, exprase in ite sith fac be buse as por is  \n",
      "----\n",
      "iter 56000, loss: 24.968760\n",
      "----\n",
      " appry, MA on the past adding the rested matrid MF model can cant bar meters tar mothls, algor thn thrergsed to the metadata into matricesting atted. Conel. Soto be, we dre in which we bace in tas nata \n",
      "----\n",
      "iter 57000, loss: 24.817726\n",
      "----\n",
      " pproal almode information is to find our ent hivies that we nate fow wat in she chape the content vares throrg. Lepadata That we content facy bat be conss case ffrm approach outst. Cony valuates of co \n",
      "----\n",
      "iter 58000, loss: 24.780740\n",
      "----\n",
      " nse of sampere woind factorization handred retadata into matabo purther jostem means that we maty the toby of the meth that matrix I cactods are deakn wass is dol in the genre plrer than the different \n",
      "----\n",
      "iter 59000, loss: 24.573579\n",
      "----\n",
      " n this terms of time hopples werk searchens do tow in as fyst. Nomovation similancy that I had impleiginally appried dost chene have a. Thos for this ticall. FM. rethod [ ]sims affed falging alving le \n",
      "----\n",
      "iter 60000, loss: 24.586458\n",
      "----\n",
      " demeating can be bot deen the Marm liter of scalations of nowodes performs the casteme wsidg to nes. rate the information from the than muth cased Bata. FR hthes when we content This with the sserform \n",
      "----\n",
      "iter 61000, loss: 24.307956\n",
      "----\n",
      " icen for a particular users and mod whithic shap astratule witadata as welleretaste facculomes instomate finanted factorization machines when tensle the ageviegitullacsularieg intent dation, Rearched  \n",
      "----\n",
      "iter 62000, loss: 24.380349\n",
      "----\n",
      " uc goin the information wime teltict valualy tebs len the dated thesfoc land gey hotait metadata is ginealy atata. It have noticindenss of pata. Burther space metarate this approach wasing these value \n",
      "----\n",
      "iter 63000, loss: 24.076867\n",
      "----\n",
      " ffroach boouls arart all int impersed priss and restris may hod vepers leAce, recommender systems whan metere not whas for this ticadetwoms out in aling onfice of follabodata  rere values I lyst a loj \n",
      "----\n",
      "iter 64000, loss: 24.209400\n",
      "----\n",
      " torized absomsith the different tach in dic nl asstochandominally atre this stex metadata proviry the dimensionality reduction about the information Blasen Wo the inpeduinionality repormpappration.\n",
      " I \n",
      "----\n",
      "iter 65000, loss: 23.938353\n",
      "----\n",
      " plroagred methods wherged norms awe sition a proach comenkine the about cends or anot abled FCFards ocoust bate information The restares in teal of pealing”, the toving as text Far methind MCGC. forth \n",
      "----\n",
      "iter 66000, loss: 24.072271\n",
      "----\n",
      " each spacing ty the hyprid factorization. SVMs. They have the Mollaborative method information about the movie morthes, sencerestective twerfed which tubular s alse data bus fel to adding the shabe th \n",
      "----\n",
      "iter 67000, loss: 23.756350\n",
      "----\n",
      " udd of tuth content based of ochestic model similes there are based of a mavel. I treptering algorithms metadata abret the behavate fecommeduates for kitairaly way word with festrithms approach: same  \n",
      "----\n",
      "iter 68000, loss: 23.894279\n",
      "----\n",
      " ape try woth. FFMC, momoves through them to searle gooving Aly, it is very porst a more dimerstles mow ration which arsof the movies. Thubo injut the ertienclust teinformalures a vends we dike meropar \n",
      "----\n",
      "iter 69000, loss: 23.535750\n",
      "----\n",
      " . Also unAperon sedented the information by whe user has a caule the data is also . I I simity. In genent. Also In , becalles we cruprocess tactent metadata, the SUstrion. Bursing this approach is som \n",
      "----\n",
      "iter 70000, loss: 23.739701\n",
      "----\n",
      "  that they wether Co velt metadata. It wends. Cowsth omatcinitimizent based on the basic dimen cascable is whice, we contune himular movies. The fialiked cangex that ster datcom Sillaring to previousl \n",
      "----\n",
      "iter 71000, loss: 23.344839\n",
      "----\n",
      " the predactors, dist its Gr ur items to a betwon work beitheir time. Therefore it campards of coukd bitadata thathy inding this searches and the previousy there ard te-pottent vs Noth all the conterat \n",
      "----\n",
      "iter 72000, loss: 23.487009\n",
      "----\n",
      "  of users have r, is factorized a got mescessing of the equation wrice the predictions erother users are added the algorsed to minimized to cleve models eatupe is morks usearzantages to increcommentin \n",
      "----\n",
      "iter 73000, loss: 23.154796\n",
      "----\n",
      " ctusterization.\n",
      "There is also tried to matrix factorization. Collowe how of readdengovs and weatut re ruch keant gigimms, “tuid, to betamata, as . As in encludad perspered therprepures to tent canes a \n",
      "----\n",
      "iter 74000, loss: 23.216936\n",
      "----\n",
      " unhor X meteracionserita—are t interent from the information from thay vallabr and spaces ensimare of ucuse thist ane users. There canal cry movies. In   we save over the probabirser a h factorization \n",
      "----\n",
      "iter 75000, loss: 22.984211\n",
      "----\n",
      " tadata that can be uphorghing the metadata intion o  trid of vating the Grerefter we adel to siknalize them a fas visur, FMMs. Ne havict, it matirifforatudes a lotime. Therefore it step as for this is \n",
      "----\n",
      "iter 76000, loss: 22.951078\n",
      "----\n",
      " ctor sugsors walch then RG Mis pr betting the factoral predimed by lisp Aption. It metecad by hed to advist. But as ard” The FMenal fepersedveddecensly addition our hist rithod could of computate info \n",
      "----\n",
      "iter 77000, loss: 22.758884\n",
      "----\n",
      " ris approach wricess fincul  approvat hied to matrix factorization. The similar tatano tantiowhes than I teed, to Therefore it wecorge toke. Therefot ins the bestions to o colmabuian ingeredglise this \n",
      "----\n",
      "iter 78000, loss: 22.685360\n",
      "----\n",
      " urtheico intorath the similarity matrix is calcunategrmsest thrse prediomor futeration which han peens, which usedste, effica ataland. FTh heselysize hat in a dener time, there are generally through t \n",
      "----\n",
      "iter 79000, loss: 22.477524\n",
      "----\n",
      " ge. Noth. FFMs, FMs mowe. This is anse we now hata epour thied to man sethoin . The toin stecal and then we added to in the ratingly wee moom there thist the information with than the re, sizivizanid  \n",
      "----\n",
      "iter 80000, loss: 22.422659\n",
      "----\n",
      " is is lose boluse of methis that the user and alypory aticliof to its uxprizatid sodd it does wul wo SrD. In  andofor the user and beciacopputes hard used int besclescuse chusticks eest hased of a miv \n",
      "----\n",
      "iter 81000, loss: 22.301987\n",
      "----\n",
      " te col neworn the sien- This way cot by have fatrix. They wellm be mallis, First plisensing the preproos the genres. The ent way bot limilar the model. The fable of the oth at. We west rest we hely we \n",
      "----\n",
      "iter 82000, loss: 22.234959\n",
      "----\n",
      " nsent on the SGDser rementaras prop I have cluscenoty the users gontext vis par moall the metadata intimpl initiall\n",
      " re convent cased and thy higFly. The fors niliming the actormatagclis. As addrouliz \n",
      "----\n",
      "iter 83000, loss: 22.099190\n",
      "----\n",
      " appries dyperientta pe frowest change as avert danabe of tampest veeral in eacteriatickicat wilimed Ind be tit what reans that the firghectinge, the user and peems are fredstell re to mimic the actera \n",
      "----\n",
      "iter 84000, loss: 22.005670\n",
      "----\n",
      " biors paty. Fegrid user aing the ass approaches the topate it por uysis iPsems of the mived to add to its us we abser. K, and spere user’sed in search. Also the simension fimensional hy, thile is give \n",
      "----\n",
      "iter 85000, loss: 21.942719\n",
      "----\n",
      " pser the results tha pertent bas mesce haddowards we have equatre refow inerquation. Tas bee potl. Bug to a simplest all broged to the base datrid factorization algorithm  sech ratie, the sim, ryst he \n",
      "----\n",
      "iter 86000, loss: 21.813700\n",
      "----\n",
      "  handiced on densitulemente out in all tre ral peesule thoin similar.  Tonot this seas stoblende werdod Fig= is to metadata fretted eatl a grodect or explicit dies wiven by pan bewenot collaboviens th \n",
      "----\n",
      "iter 87000, loss: 21.811032\n",
      "----\n",
      " d approach the perint calluloved by SGD angslow haw througher matrix that - users and tried to mind is t I viener through there wailities which hrot the regules the waich ueplysed east contend approac \n",
      "----\n",
      "iter 88000, loss: 21.596185\n",
      "----\n",
      " ice product, Rearmstrg in ralce fienimilar telllantically Berduction. User-Usenoty ind that we see hifferent form in ourestichrgent has been a lot of evifisar welus to Cogel mean [ byime of tle inform \n",
      "----\n",
      "iter 89000, loss: 21.642569\n",
      "----\n",
      " ucon twis also information from hes imforts go interalumensit to predict the raselizedic. The reason gs of tas calues farltically as gel notalles surgerof Neterrs impuet syst infor above wot betoby ve \n",
      "----\n",
      "iter 90000, loss: 21.430967\n",
      "----\n",
      " fon efficher a mow intors these difner anon model rimention amorm it has abrovis, recommender systems betweentrailted. Therefore abunt point speclios this tich re to mike Finst I ly betadet gr Colk in \n",
      "----\n",
      "iter 91000, loss: 21.504469\n",
      "----\n",
      " els. The mame similarly. The different waseghace of filar stast. Furss oform, in working agrigies and dimical wense mas aknt   trouthm Adsthedictiond the base data. We are arat in a lething approache  \n",
      "----\n",
      "iter 92000, loss: 21.342186\n",
      "----\n",
      " plith seed, by Using search: Neth exing als hybrix that would to gogur whece been the metadata for in the movies orendby the meaksing the himensionality Reduction the r-sulted to manre which is noly f \n",
      "----\n",
      "iter 93000, loss: 21.418653\n",
      "----\n",
      " edimples a vaster and pas nothod in recommender systems haved abous fell watch tesprixel. Nosically aptrian werl abovationallidformation. Also kes unfor ithor values fiveny is por insteale which that  \n",
      "----\n",
      "iter 94000, loss: 21.213576\n",
      "----\n",
      " nger anoreduct on tast it most. The fach procest ppred porthesporys entlust is to a colculize it doed that it gecomata. As  ithir slacabout the legre out if with cester is product has ustrased ratings \n",
      "----\n",
      "iter 95000, loss: 21.308941\n",
      "----\n",
      " ally in the complicit to prith it . Itext Predictoracteristicselvance of in terms of upproach This approach re some. The values fuctulamorged are dirs opplexity the convergence rating the topares that \n",
      "----\n",
      "iter 96000, loss: 21.068353\n",
      "----\n",
      " ative alsoractions of thexififur which the performancute tors G. There also todules us to see extrain novease predictions which a parinarize throces us the same seen. Allelial lyper-paralcorithms are  \n",
      "----\n",
      "iter 97000, loss: 21.225581\n",
      "----\n",
      "  them towards of noto preding them ally mowelen the probabel. Buriee in gintandich censing lasen in item users and mitht rmatic phoss but a lescagings fors not peeduct the first incets try ve [ sparse \n",
      "----\n",
      "iter 98000, loss: 20.941745\n",
      "----\n",
      " the gethichirg that we going so mot bas ffostermonl and collablend of the hyper-profactorization and with in that the ferformawion kifferent it mactor spaly us the sybollated parsoracters. I needed to \n",
      "----\n",
      "iter 99000, loss: 21.077365\n",
      "----\n",
      "  in tallor spand or velar imilal caine the different algorithms and cauler the pate overy seris \n",
      "s accolatens. These aretadate in hecout whicust busid purse. In the user ration shain nel [ tablid MFco \n",
      "----\n",
      "iter 100000, loss: 20.809513\n",
      "----\n",
      " than the user. For extugutemsition ot gives us adding to that they allor than I fardows. We handle soalso matrix factinuly between arcom gorithll the call of the grid. There is a betternng it in the f \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mW1, mWh1, mWhh, mWh2, mW2 = np.zeros_like(W1), np.zeros_like(Wh1), np.zeros_like(Whh), np.zeros_like(Wh2), np.zeros_like(W2)\n",
    "mb1, mbh, mb2 = np.zeros_like(b1), np.zeros_like(bh), np.zeros_like(b2) # memory variables for Adagrad                                                                                                                \n",
    "smoothLoss = -np.log(1.0/vocabLen)*seqLength # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seqLength+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hiddenLayer,1)) # reset RNN memory   \n",
    "        h1prev = np.zeros((hiddenLayer,1))\n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [characterToIndex[ch] for ch in data[p:p+seqLength]]\n",
    "    targets = [characterToIndex[ch] for ch in data[p+1:p+seqLength+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2, hprev = propagate(inputs, targets, hprev, h1prev)\n",
    "    smoothLoss = smoothLoss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter %d, loss: %f' % (n, smoothLoss)) # print progress\n",
    "        sample(hprev, h1prev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([W1, Wh1, Whh, Wh2, W2, b1, bh, b2],\n",
    "    [dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2],\n",
    "    [mW1, mWh1, mWhh, mWh2, mW2, mb1, mbh, mb2]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learningRate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seqLength # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
