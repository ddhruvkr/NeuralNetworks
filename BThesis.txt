In the past years, recommender systems have been used in many web applications to predict products, movies, songs, etc. for the users. The basic task is to figure out the top items that the user might like. In recent years, it has also received a lot of interest from the academia. There has been a lot of interest shown in a collaborative filtering algorithm called Matrix Factorization as it has demonstrated a better performance than the other Collaborative Filtering and Content Based Filtering Algorithms. The former approach looks at the relation between different items and different users and tries to predict on the basis of this similarity, whereas the latter looks at the information metadata provided about the items and the users and tries to predict on the basis of this. Thus far the hybrid methods of collaborative filtering and content based approach have not been successful in integrating content based metadata for improving the performance. My goal in this project is to test a hybrid approach and test how well it is able to integrate the metadata in improving the performance of the system and then looking into the behavior of a state-of-the-art model called the factorization machines when the metadata is added to it.
 Introduction The users nowadays spend a lot of time online, searching for products and information. Therefore it has become a task of utmost importance to make their searching efficient by helping them with information that is useful to them and eliminate the information that may not be valid for a particular user. For example, YouTube recommends videos to its users based on their previous searches and the videos they had watched previously as well as from the channels one has subscribed. Similarly, movie and TV streaming website Netflix, recommends movies and TV series to its users based on their watching history. If the users don’t waste their time on searches that are not informative to them there would be increase in the profit that the providers make. This is the reason so many of the online retailers are using this technology. It tends to add an extra dimension in the user’s experience. If the user has a good experience, there is a chance that he would return to the website. This has therefore become the key to the user’s satisfaction and their loyalty towards the service providers. Therefore almost all of the service providers have begun studying and analyzing the user patterns and trying to include this information in predicting items to their users. 
 Problem Definition In my study, I have n users and m items. These users have rated these items and we have these explicit ratings r. We have our data is in the form of user, item, rating <u, m, r> tuples. Beside this we have some additional information in the form of content metadata about the items. We have a short summary about each movie. Further we have the information about the cast of the movie including the actors, producers, director, etc.  Auxiliary data There are perspectives to look at the auxiliary data metadata that can be used for improving the performance of the recommender systems. User generated vs expert: There is some information that is given by the user about the product or even himself. This include ratings given by user, reviews written by him and click history. There is also this other category which includes information entered by an expert like a description about the product, relevant tag given and general information about the product.
 Content vs Context vs Network vs Feedback: The other perspective is given in which classifies metadata into categories  Content metadata- User’s static profiles, Item’s description Context metadata- User’s dynamic mood, information of time  Network metadata- Social relations of users, friends Feedback metadata- feedback given by the user, browsing history of particular item For the rest of the paper I am going to follow the second perspective about the metadata.
 Goal My goal then is to find an efficient way both in terms of time and space measures to use this information to improve the accuracy of recommender systems. In my case the auxiliary data that we would be using is the content metadata about the product. The major challenge is to extract some useful information from this and then to bring it to a form in which it is useful to the models. First I test a way to incorporate the processed information from the metadata into matrix factorization and then I test how factorization machines process the same information.
 Non-personalized approach This was the first and the least complicated approach towards recommender systems. Through it the same recommendation is given to all. They take in all the data that they get and generate recommendations like “bestseller”, “trending”, etc. They do not require any personal information regarding the user. Content Based Approach This approach requires a lot data that the user may have to manually enter. This data includes information about the user as well as the movies. The information about the user might include demographic information or the user filling a questionnaire. The information about the movie might include explicitly which genre it is, its box office popularity, etc. This model thus builds profiles for users and movies and then tries to match these profiles to predict the movies that a user might like . The issue with this approach is that getting all this information from the user and also about the movie can be a very hard task. One more drawback is that by using this approach the system is restricted to recommending the same type of movies that a user is already watching. Collaborative Filtering Approach It is quite a popular recommender system technique which uses the information from other users to predict the rating for a particular user. The underlying intuition that it uses is that we can find similar users and then use similarity between them to find people with similar tastes and use that to predict the movies that a particular user might like. For instance, suppose that a group of users have a similar taste with a person X, then we might suggest this person X the movies which these group of users have rated highly and the user X has not seen them. There are various types of algorithms within Collaborative Filtering. Memory Based Nearest Neighbor  . User-User CF This is also known as k-Nearest Neighbor CF. It was first introduced in the GroupLens Usenet article recommender [ ].  The basic philosophy is finding the k most similar users and then using this to predict the movies. First a similarity matrix is computed between the users. There could be many ways to calculate this like using Pearson Correlation, 
Cosine Similarity, etc. Then the rmse values for the movies for a particular user is calculated using these similarity matrix and finally the top movies based on these rmse values is served to the user. Pearson Correlation has been found to be the best method [ ][ ]  . Item-Item CF Although the User-User CF method gives accurate results but it suffers from a problem of scalability as the number of users grows. To deal this problem this approach is used. In this approach instead of finding the similar users, we find the similar movies. Item–item collaborative filtering was first described in the literature by Sarwar et al. [ ] and Karypis  . If two movies have been watched by the same group of people and they all tend to feel similar about a movie then the movies are similar. The similarity matrix is calculated in the same way as in the User-User CF method and then the top movies which are similar to the movies that the user has rated highly is served to the user. The main advantage of this method is that once many users have given the ratings for an item then we can pre-compute this similarity matrix and it saves a lot of time. Even when a new user comes and rates a product there would not be much change in the similarity matrix. The similarity can be re-computed in some fixed intervals of time to account for the new ratings or the changed ratings, but the interval would not be that short. Model Based Latent Factor Models These have proved to be superior to the previous class of Collaborative Filtering algorithms in terms of the predicted accuracy. In this model the factors are these hidden characteristics of both users and movies. These factors are learnt by the system and based on the dot product of these factors, the ratings are predicted. Hybrid Approach This approach mainly seeks to combine the advantages of both content based and collaborative filtering approaches. Also models that use different techniques all of them from a same approach could also be called a hybrid model. One way in which the hybrid systems are used are by adding content based capabilities to a collaborative filtering system. We have used this approach in the first part of our research. We tried to incorporate additional information from the metadata to the matrix factorization model. Context Aware Approaches In general the recommendation systems are not good in taking into account the extra information like the time, place, weather, etc. when a product was selected. The challenge in adding this type of information is that the feature space increases. Already the recommender systems have problem dealing with the sparsity of the information that we have. When extra dimensions are added the already sparse feature space becomes 
sparser and becomes a very big problem. Therefore we need efficient algorithms which can deal with such large sparsity. Models like tensor factorization and factorization machine can handle the above mentioned issues.  .  Related Work A lot of research has been done previously to incorporate different type of metadata to recommender system models earlier. Basically as described in   recommender systems are broadly classified into categories which are content based, collaborative filtering and hybrid approach. As we see in  ,   creating a hybrid approach by adding content metadata information to matrix factorization methods have not been so successful in the past. In   we see how time related metadata can be used in collaborative filtering. In   we see how user and product profile are built by using user reviews feedback as source of extra information. This is where tensor factorization methods  ,   gained popularity as they were able to better handle this extra contextual information by doing factorization in more dimensions. Then Rendle in   introduced factorization machines. They include polynomial regression with factorization techniques. They in almost all cases perform better than tensor factorization. They are known to mimic famous factorization models like Matrix Factorization MF [ ] and Bayesian Probabilistic Matrix Factorization BPFM  . As shown in   they are also better to suited to handle extra contextual information. In general user generated or collaborative metadata is found to be more useful in improving the performance of recommender systems than say content metadata. In   the authors talk about implicit knowledge about user’s action of browsing and clicks feedback metadata can be used to further enhance the performance of factorization machines. In   the authors talk about that training the model not all data but on a right slice of data. The slice is chosen by grouping items based on their properties. In   the authors have tried to utilize additional explicit feedback metadata like binary like/dislike ratings through factorization machines. But as we see not much work has been done in analyzing how factorization machines handle content metadata.
This method comes under the Latent Factor Models of the Collaborative Filtering Algorithm. It has now become the basis of every standard Recommender System machine as it can handle both implicit and explicit data and gives pretty accurate predictions. It was proposed by C Volinsky et al [ ]. Matrix factorization technique maps the user and items to a joint latent factor space of a fixed dimension and then the user item 
interactions are obtained by interaction between the latent factor spaces. It is the preferred method over SVD which has problems dealing with incomplete matrices. The user-item relation is mapped to a fixed dimension f by singular value decomposition of the ratings data.
In this way each row of P would represent the strength of the associations between the user and features and similarly each row of Q would represent the strength of the associations between the movie and features. Now our main aim is to map the user, item, rating tuples from the training set to the joint latent factor space of dimension f. The problem is that in general users tend to rate very few number of movies. Therefore we have a very sparse matrix in R. Therefore we need to add the regularization term in our equation to prevent overfitting of data. The equation that we need to minimize is 
While in general stochastic gradient descent is easier and faster than ALS, ALS is favorable in at least two cases. The first is when the system can use parallelization. In ALS, the system computes each Qi independently of the other item factors and computes each Pu independently of the other user factors. This gives rise to potentially massive parallelization of the algorithm. The second case is for systems centered on implicit data. Because the training set cannot be considered sparse, looping over each single training case—as gradient descent does—would not be practical. ALS can efficiently handle such cases
 Proposed Approach As explained in section  . , I have chosen the Matrix Factorization as the base of my approach. There also have been few proposed approaches for this like  . The basic purpose of the following algorithm will be to try to include metadata into the Matrix Factorization and see if it helps us to improve the accuracy even further.  .  Type of data I have used MovieLens  M dataset for the ratings of the movies given by the user. It has million tuples user, movie, ratings. It has  users and  movies. I have taken the metadata from DBpedia. It has abstracts of the movies and also has the directors, producers, actors and writers in the movie.  Algorithm Adding Genre Information The first task is to find out something from the abstract that could be useful for our Matrix Factorization approach. I have tried to figure out the genres of the movies through the abstract given. The most basic approach could be to get a list of genres and then search for these keywords in the abstract. But this approach has certain shortcomings. For example, if a different form of the word genre is present in the abstract it would be missed. For example, if the genre of a movie is “History” and the word “historical” appears in the abstract, it would be missed. Therefore, I have stemmed the genres and brought them to their root form, so even if there is a word with a different form it can get labelled. Also I have added certain keywords in the genre list like “award”. The reason behind this is some users are affected by whether a movie was in the contention to win an award. After this step is implemented we get a MG movie x genre matrix. This is a binary matrix which tells whether a particular movie has characteristics of a particular genre or not. We then normalize this matrix by dividing the values by which represents the total number of genres in a movie. This step is necessary as there may be more than one genres of a movie and the user may like a movie more because of a particular genre.
 In this part I tried to incorporate the information of the actors, producers and writers in the movies to the algorithm.  . . .  Dimensionality Reduction Approach First I tried adding the information in pretty much the same way as the genre information. The main difference is that in the case of genres the matrix MG size was very less as the size of the genres list was , but in this case the size of matrix MA is very large as the number of actors, producers, etc. is  actor count. Therefore, it is very 
computationally intensive. To solve this problem, I applied dimensionality reduction over this matrix MA to reduce the dimension to  . Then the matrix UA user x actors is calculated in the same way as the matrix UG was calculated previously through the equation. Clustering Approach In approach  , we applied dimensionality reduction, but one issue that it faces is that some information is lost. Therefore through this second approach I propose an algorithm through which no information is lost and we can reduce the complexity that we face. I have opted to use clustering for condensing the information as it does not cause any loss of information. So now initially we have the MA matrix which is very large in size. Then I calculate a matrix AG actor x genre which basically tells us what is the preference of the actors, directors, producers, etc. with respect to the genres. Then through this matrix I calculate a similarity matrix between actors, simA. The measure I used to calculate the similarity was cosine. The basis was the genre. The cosine similarity can be put as in equation.
ering can be applied on this matrix to group similar actors, directors, producers, etc. together. There are many algorithms that could be applied for clustering. I tried with the following two:  . Agglomerative clustering: Part of the hierarchical clustering strategy, it is a bottom up approach. Each observation starts in its own cluster and pairs of clusters are merged as one moves up the hierarchy. In general, the merges and splits are determined in a greedy manner. The results are usually represented in a dendrogram. I used the scikit learn agglomerative clustering algorithm for implementation. K-means: The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. The k-means algorithm divides a set of  samples  into  disjoint clusters C, each described by the mean  j, of the samples in the cluster. The means are commonly called the cluster “centroids”. The K-means algorithm aims to choose centroids that minimize the inertia, or within-cluster sum of squared criterion.
Factorization Machine Factorization Machines   are a new model class that combine the advantages of SVMs with factorization models. Like SVMs, FMs are general model predictors working with any real valued feature vector. In contrast to SVMs, FMs models all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity like recommender systems where SVMs fail.  There are many factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models are that they are not applicable for general prediction tasks but work only with special input data. Furthermore their models equations and optimization algorithms are derived individually for each task. FM can mimic these models just by specifying the input data.  The FMs can be compared to SVMs with a polynomial kernel. SVMs define a multidimensional hyperplane, which learns the shape of the curve of the data. However, SVMs have certain weaknesses which are addressed by the FMs. For example, the SVMs do not work well on sparse data. Further, in SVMs, the input variables are still independent variables even though the polynomial kernel attempts to model the interaction among the variables. This takes polynomial time to compute.  The advantages that FMs have over the SVMs are the following: . They allow parameter estimation under very sparse data. In fact, they are built specifically for sparse data. They do not perform well on dense data.  . They have linear complexity, and can be optimized in the primal and do not rely on support vectors like SVMs.  . They can model n-way variable interactions, where n is the number of polynomial order. Although in general the value of n is kept in most cases.  There are several implementations of factorization machines. The state-of-the-art continues to be libFM  . In the original paper  , Rendle discussed a method of optimization for the model parameters known as stochastic gradient descent, which works well with several loss functions. However, the optimization algorithm is extremely dependent on the learning rate, one of the hyper-parameters of the method. If the learning rate is too high, the model parameters will not converge, while if it is too low, the algorithm is no longer time-efficient. Because of this, Rendle reviewed three more methods known as alternating least-squares  , markov chain monte carlo inference   and adaptive stochastic gradient descent 
 . He recommends markov chain monte carlo inference because there are fewer hyper-parameters, and those that must be specified are not as sensitive to their initial values. 
This method tries to mimic the Bayesian Probabilistic Matrix Factorization BPMF method. This means that we add a Bayesian inference over the probabilistic matrix factorization. This means that we set hierarchical priors over the hyper-parameters to regularize them. The intuition behind this is that additional hierarchical structure put on model parameters shrinks them towards each other as long as no evidence in the data clearly indicates the opposite. The figure below compares the probabilistic interpretation of standard Factorization Machines to Bayesian Factorization Machines.
To calculate the average rmse I have take the mean value of the calculated rmse’s from different cross-validation splits. fold - cross validation split has been used. There were some inconsistencies with the movies in the LastFm dataset and MovieLens dataset. So only those movies were taken for which the additional metadata information was given. After reduction the total number of movies left were . The tuples which contained the movie whose abstract was not given were removed. Finally after running the algorithm on different cross validation runs the mean rmse was calculated.
There are generally most common methods to tune hyper-parameters Grid Search: It picks out a grid of values and then evaluates every value and then returns the value which gives the best result. Some guesswork is necessary to aim the algorithm to search in the right direction. It is the most expensive method in terms of computation time but we can reduce it by parallelizing it. Random Search: It is a simple solution which is surprisingly effective. It is a slight variation of grid search. Instead of searching at every point in the grid, it searches only at some random points in the grid. This makes it vary cheaper in terms of computation time. As it does not searches the entire grid therefore it does not beat grid search. But as shown by Bergstra and Bengio[ ] random search performs the same as gird search. All in all trying  random points sampled from the grid should be good enough. Smart Hyper-parameter tuning:  Instead of preparing a batch to search initially, it picks some points, evaluates them and then decides on the basis of the evaluation the next points to search. Therefore it cannot be parallelized. There is a lot complexity in implementing these smart algorithms and making them run faster than random search. They have their own hyper-parameters that need to be tuned to run efficiently. 
I have used grid search to tune the hyper-parameters initially using some the standard values to start and then building on that as I went ahead. The following table shows the value of tuned hyper-parameters for the hybrid matrix factorization.
Baseline Prediction First to have a baseline for comparison, I evaluated the following algorithm. Let bui denote the baseline prediction of user u for item i. The simplest method could be to predict the average overall ratings in the system: bui =  mean. This can be enhanced by further calculating the average rating for user bu and item bi. The final enhancement that can be done is to combine them all to predict the rating. The formula for this is as follows
The comparison is made between the MF with the proposed algorithm which includes information from the metadata as well. This comparison is shown in table which has the mean rmse values given by the different algorithms. MF represents the basic matrix factorization, HMFG represents hybrid matrix factorization with genre metadata, HMFC  represents hybrid matrix factorization with cast information dimensionality reduction metadata, HMFC  represents hybrid matrix factorization with cast information clustering metadata, and HMFGC represents hybrid matrix factorization with cast and genre information metadata. As we see from the table, adding genre information improves the RMSE. Out of the two approaches to add cast information, the clustering approach outperforms the dimensionality reduction approach. This is as a lot more information is lost when we apply dimensionality reduction. Adding all the information further improves the result. Also we have fig. and which serves as a visual tool for comparison between different algorithms. This visual tool helps us to visualize the different rates at which these different algorithm converges and then we can use the most appropriate approach
As we see in fig.  , the clustering approach is much better than the dimensionality reduction approach from the start. It converges quicker. The reason for this is that a lot more information is lost when we apply dimensionality reduction than that in clustering. As we see from fig.  , the MFG algorithm is the fastest algorithm in terms of the convergence rate initially. But finally the MFA_cluster beats it in the RMSE values. Thus we see that our hybrid MF model with content metadata added to it performs better than the collaborative matrix factorization. But the important thing to see is that the improvement is very little. This matches with the previous research that has taken place in this field of adding content metadata to MF method. To overcome this as discussed above other methods multiverse tensor factorization and factorization machines have been used. In the next section we see the results that we got when we added the same content metadata to the factorization machines.
 The same data was added to the factorization machines as to the hybrid matrix factorization. The results are computed for the different approaches through which factorization machines can be implemented and with all the different metadata that can be added to them. As approach for adding the cast metadata was not effective therefore in this case we only consider the approach in our results. The following tables show the results for different approaches and with different information.
 The first thing to note that the improvement that we got after adding metadata to factorization machines was much more that the improvement we got after adding metadata to matrix factorization. The SGD approach here is what resembles that matrix factorization that we implemented earlier. The thing to note here is that the rmse value is almost the same  . for the raw data earlier and  . when implemented through factorization machine. Therefore, we can conclude that factorization machines are good at mimicking the models.  As we see in the improvements are quite significant as compared to the improvements we got through matrix factorization. Table shows the percentage improvement that we got through different approaches. This percentage is calculated by the following formula 
 As we see the percent change for the MF method which we discussed earlier is much less than the SGD, although both methods are essentially the same and give the same rmse values on the base data, but behave very differently when metadata is added to it. When we compare the different algorithms that were implemented through factorization machine, MCMC performs the best, followed by SGD and then by ALS. Also in terms of percentage change, the most change is recorded in MCMC, followed by ALS and then by SGD. In terms of time and iterations taken to converge, ALS is the fastest followed by SGD and MCMC. The figure shows this point. The ALS algorithm converges in about - iterations, SGD takes about - iterations and MCMC takes about  -  iterations to converge. The figures shown below gives us a visual representation of the different models. Fig. is the basic comparison of the algorithms implemented through the factorization machine. It gives us an understanding towards us how the algorithms behave in terms of rmse and the convergence rate on the base data. We are able to see that MCMC is the best of the three. Fig.  , and show how the three algorithms behave when different pre-processed metadata is added to them. The data when all the information from the metadata is added gives us the best result in all the cases. 
 When I had only implemented the matrix factorization model, the following were the questions that I needed to address. I needed to find the reason why matrix factorization was not able to improve the results to a significant amount. I needed to answer whether the reason for this the algorithm itself, the poor data quality or in the way I was handling the metadata itself. I also needed to look for an algorithm which could work better than the matrix factorization in its base form and also could handle the information from metadata more efficiently. Now let us address the behavior of the factorization machines that we have seen. As we already saw that the factorization handle the metadata pretty well. This is due to the fact that they are meant to handle sparse data. Our earlier model struggled to make much sense of the metadata as we were the information which was also sparse apart from the sparse base data. But in case of factorization machine which can make sense of sparse data, this was not the case. It was able to make connections when we added more sparse data.  One important thing about the factorization machine is that when we added the metadata that we got through applying the approach on cast information the rmse values increased instead of decreasing. This was due to the fact that the processed data that we get after applying the approach  , is not sparse in nature. This makes the performance of factorization machines to decrease which are not able to handle dense data. When we added the same data to the matrix factorization algorithm, the performance did not improve a much but also it did not deteriorate as well. Now let us look how important did the preprocessing that we did with the genre and cast information prove to be. To test this we did two tests. First, instead of adding the processed metadata we tried to the data in the raw form. This means that in each row instead of adding the processed value which was between and  , we now added either a or a  . This means that if a genre or a cast is present in a movie then we put the binary value else the binary value  . The table compares the result when we added the processed knowledge about the cast of the movie to the results we got when we added information about the cast in the unprocessed form in its entirety. As we see it performs worse than the results that we got when the processed information was added. Not only that, the results are slightly bad than that we got with no metadata information. In the second test we added random values to the factorization machine similar to that as our metadata. The sparsity of the data was the same as that of the metadata. We observed that the rmse value was greater than that we got when no metadata was added. Both these tests indicate that the preprocessing with the data is a crucial step.
Therefore now I can answer all the questions that I had after we had implemented just the matrix factorization algorithm. The matrix factorization was not able to handle the metadata as it was very sparse in nature. The data was not that bad as we thought it was initially. The preprocessing steps that I applied were working well, so I was actually handling the metadata in the right way. I found a better model than matrix factorization in MCMC. The Libfm implementation was much faster than the matrix factorization model that I had implemented ourselves. I was also able to analyze how factorization machines reacted to different type of information that I incorporated and I was able to figure out what works well with them. I also try to add the information about the user that I had which included the age, occupation and gender of the user. I just added it to the FM as it was because there was nothing to process in this type of data. The table shows the results. 
In this thesis, I tried to test how well factorization machines can handle the content metadata. It is already shown that factorization machines work well in integrating contextual metadata without much preprocessing. We concluded that it can handle the content metadata extremely well and better than the other models given the data is preprocessed. For future work I propose two areas to look into. Firstly user side information can be processed and added to see how factorization machines use them. Secondly, as in, I can look into techniques through which I can calculate the similar words for different tags. One such technique could be natural language processing as this would help us make more sense of the synopsis of the movie. As of now, I am just searching for the root forms of some critical words, but if I could generate other words similar to tag words, I would be able to better judge the preference of the