{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "totalCharacters = len(data)\n",
    "vocabLen = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'p': 2, 'c': 3, 'z': 4, 'N': 5, 'x': 6, 'I': 7, '\\n': 8, '?': 9, 's': 10, 'S': 11, 'l': 12, 'F': 13, '\"': 14, 'm': 15, 'U': 16, 'k': 17, 'j': 18, 'P': 19, ',': 20, 'T': 21, 'H': 22, 'w': 23, 'y': 24, 'e': 25, 'J': 26, 'C': 27, 'L': 28, 'M': 29, '(': 30, 'r': 31, '!': 32, 'o': 33, \"'\": 34, 'q': 35, 'E': 36, 'O': 37, ' ': 38, 'n': 39, 'W': 40, '-': 41, ':': 42, 'f': 43, 'รง': 44, 'v': 45, 'V': 46, 't': 47, 'u': 48, 'G': 49, 'Y': 50, 'i': 51, 'Q': 52, 'd': 53, 'A': 54, 'D': 55, 'g': 56, 'h': 57, ')': 58, 'B': 59, '.': 60, ';': 61}\n",
      "{0: 'a', 1: 'b', 2: 'p', 3: 'c', 4: 'z', 5: 'N', 6: 'x', 7: 'I', 8: '\\n', 9: '?', 10: 's', 11: 'S', 12: 'l', 13: 'F', 14: '\"', 15: 'm', 16: 'U', 17: 'k', 18: 'j', 19: 'P', 20: ',', 21: 'T', 22: 'H', 23: 'w', 24: 'y', 25: 'e', 26: 'J', 27: 'C', 28: 'L', 29: 'M', 30: '(', 31: 'r', 32: '!', 33: 'o', 34: \"'\", 35: 'q', 36: 'E', 37: 'O', 38: ' ', 39: 'n', 40: 'W', 41: '-', 42: ':', 43: 'f', 44: 'รง', 45: 'v', 46: 'V', 47: 't', 48: 'u', 49: 'G', 50: 'Y', 51: 'i', 52: 'Q', 53: 'd', 54: 'A', 55: 'D', 56: 'g', 57: 'h', 58: ')', 59: 'B', 60: '.', 61: ';'}\n"
     ]
    }
   ],
   "source": [
    "characterToIndex = {ch:i for i,ch in enumerate(chars)}\n",
    "print(characterToIndex)\n",
    "indexToCharacter = {i:ch for i,ch in enumerate(chars)}\n",
    "print(indexToCharacter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#hyperparameters\n",
    "learningRate = 0.01\n",
    "hiddenLayer = 128\n",
    "seqLength = 100\n",
    "#modelParameters\n",
    "#connect input layer to hidden layer\n",
    "W1 = np.random.randn(vocabLen, hiddenLayer) * 0.001\n",
    "# connect hidden layer to output layer\n",
    "W2 = np.random.randn(hiddenLayer, vocabLen) * 0.001\n",
    "# connect hidden layer to hidden layer in the next timestamp\n",
    "Wh1 = np.random.randn(hiddenLayer, hiddenLayer) * 0.001\n",
    "Whh = np.random.randn(hiddenLayer, hiddenLayer) * 0.001\n",
    "Wh2 = np.random.randn(hiddenLayer, hiddenLayer) * 0.001\n",
    "b1 = np.random.randn(hiddenLayer, 1)\n",
    "bh = np.random.randn(hiddenLayer, 1)\n",
    "b2 = np.random.randn(vocabLen, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the loss function would take in the input chars, the output chars and the previous hidden state\n",
    "# it outputs the hidden state, the gradients for each parameter between layers and the last hidden states\n",
    "def propagate(inputChars, outputChars, prevH, prevH1):\n",
    "    x, h, h1, y, p = {}, {}, {}, {}, {}\n",
    "    #x = the array which is a list of zeros, with just 1 at the index where input character is\n",
    "    #h = values of hidden layers at different times\n",
    "    #y = values of outputs not activated\n",
    "    #p = activated output\n",
    "    h[-1] = np.copy(prevH)\n",
    "    h1[-1] = np.copy(prevH1)\n",
    "    loss = 0\n",
    "    #forward propagation\n",
    "    for t in range(len(inputChars)):\n",
    "        x[t] = np.zeros((vocabLen, 1))\n",
    "        x[t][inputChars[t]] = 1\n",
    "        h[t] = np.tanh(W1.T.dot(x[t]) + Wh1.dot(h[t-1]) + b1) # h has to be of the same dimension as b\n",
    "        h1[t] = np.tanh(Whh.T.dot(h[t]) + Wh2.dot(h1[t-1]) + bh)\n",
    "        y[t] = W2.T.dot(h1[t]) + b2\n",
    "        p[t] = np.exp(y[t])/np.sum(np.exp(y[t]))\n",
    "        loss += -np.log(p[t][outputChars[t],0])\n",
    "    \n",
    "    \n",
    "    #backward propagation\n",
    "    dW1, dW2, dWh1, dWh2 = np.zeros_like(W1), np.zeros_like(W2), np.zeros_like(Wh1), np.zeros_like(Wh2)\n",
    "    db1, db2, dbh = np.zeros_like(b1), np.zeros_like(b2), np.zeros_like(bh)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    dh1next = np.zeros_like(h1[0])\n",
    "    for t in reversed(range(len(inputChars))):\n",
    "        dy = np.copy(p[t])\n",
    "        #starting the backpropagation\n",
    "        dy[outputChars[t]] -= 1\n",
    "        \n",
    "        dW2 += h[t].dot(dy.T)\n",
    "        db2 += dy\n",
    "        dh1\n",
    "        dh = np.dot(W2, dy) + dhnext\n",
    "        dhraw = (1 - h[t] * h[t]) * dh\n",
    "        db1 += dhraw\n",
    "        dW1 += np.dot(x[t], dhraw.T) #derivative of input to hidden layer weight\n",
    "        dWh1 += np.dot(dhraw, h[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Wh1.T, dhraw)\n",
    "    for dparam in [dW1, dWh1, dW2, db1, db2]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dW1, dWh1, dW2, db1, db2, h[len(inputChars)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (512,512) and (100,1) not aligned: 512 (dim 1) != 100 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e316d63c0ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddenLayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset RNN memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#predict the 200 next characters given 'a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcharacterToIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-e316d63c0ee2>\u001b[0m in \u001b[0;36msample\u001b[0;34m(h, seed_ix, n)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutputChars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#compute output (unnormalised)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (512,512) and (100,1) not aligned: 512 (dim 1) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "                                                                                                                                                                                        \n",
    "    #sample a sequence of integers from the model                                                                                                                                                \n",
    "    #h is memory state, seed_ix is seed letter for first time step   \n",
    "    #n is how many characters to predict\n",
    "\n",
    "    x = np.zeros((vocabLen, 1))\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    outputChars = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(W1.T, x) + np.dot(Wh1, h) + b1)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(W2.T, h) + b2\n",
    "        # probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #print(p)\n",
    "        #pick one with the highest probability \n",
    "        selectedChar = np.random.choice(range(vocabLen), p=p.ravel())\n",
    "        #print(ix)\n",
    "        #create a vector\n",
    "        x = np.zeros((vocabLen, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[selectedChar] = 1\n",
    "        #add it to the list\n",
    "        outputChars.append(selectedChar)\n",
    "\n",
    "    txt = ''.join(indexToCharacter[char] for char in outputChars)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "    hprev = np.zeros((hiddenLayer,1)) # reset RNN memory  \n",
    "    #predict the 200 next characters given 'a'\n",
    "sample(hprev,characterToIndex['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 412.750232\n",
      "----\n",
      " IqGAf)ASb\"YAvg: ufUNBSgbY,toq! S:AlqqqNkqS\"rYDo wNboqyY\"AYqpqq-YfNgPcWhNxSgwYDuqIy.gWbN!t Acqa;(t)NgIrbq.tbgqd-toT'YwWHzyD(taq!oHqDqvvSgl! SygVGgWoyPx:;pz zfvaNqqdoA, lvST;Oq; qoVqg:A kEwq,mwGHDHcFNNm \n",
      "----\n",
      "iter 1000, loss: 326.216718\n",
      "----\n",
      " rkpo\"e torhet Gevshetrobthor Gt ataehe co iam goatt dtsithecnd aulur tathet rhlut as thard to tir ondove tom red her lellr thethit an hitnebeywhed -ec, evot tGan erent homeewed n, endth,d tirikoink te \n",
      "----\n",
      "iter 2000, loss: 264.111515\n",
      "----\n",
      " ed ohenr fanle an paly yhots thel the cher el wed and has ango nald supg f cte hom, hy swire wther it cidiy beair The saab io wever'g ths et werrer hlo wlas to lumasisg bed on un thoy ansleat whan heg \n",
      "----\n",
      "iter 3000, loss: 230.493823\n",
      "----\n",
      " e ofker in to ole anet' ther, broored an? nen of tort baserisid catled sille saduth d wam finded\n",
      " ot the sik of for sero cang dha sore orlo la lon pove late ked of. Bfthed ther bos coht aonle tor, at  \n",
      "----\n",
      "iter 4000, loss: 212.634100\n",
      "----\n",
      " aod andt ope machit her ingithinged aud wom eratd the wiid or and wat, allerly or qok shangt reed tid was she moussed the ghsarite pedw had had nof wam sty in oot reoren's Grogir ould. Therropd in sow \n",
      "----\n",
      "iter 5000, loss: 201.907566\n",
      "----\n",
      " . To the bnutprain riin arof. Gregor'agn \"roully oo hem lettat . Yhe uat to bean bent ot hetthime wath einked gowbed. And he, ence ve hither. Tis hore anly to four an alle. \"Thet wapped orecr thele Io \n",
      "----\n",
      "iter 6000, loss: 193.558175\n",
      "----\n",
      " e wound bowd toon heesin the cout lyer so poith hes his lf ale, hay then, erong Gregor!s, in Me wowenl,, jucn theve angouse aidersiy sound lag not, at ald ai wor;. The hat sterbegol\", eith agding wann \n",
      "----\n",
      "iter 7000, loss: 186.506600\n",
      "----\n",
      " am aod him with ost ntoned csand her eplrentisto that ham and purding his havlly. Nud the ouver feke in s iatat and nenhanct puck and celligeven wat se vindedbeis-, to hil comeningte. \"ich good, the i \n",
      "----\n",
      "iter 8000, loss: 182.107825\n",
      "----\n",
      " athing ano 'r shanever wruld on thoude siste to that Gregor, whand he sinfer her with pinnst now, atoud as hiresushy forly wos stintt'ne hirs phenes, thim thobe, amfarnes in the dlofcy was ef;ilbnet l \n",
      "----\n",
      "iter 9000, loss: 178.569038\n",
      "----\n",
      " ed to the soob anding. He pay wor and bed monk when ally is ardo pries leas sos that letf ste lever beaby deom collyen the would bevs not nge had po moqmed rot and be't promeded myim was and hes bagk  \n",
      "----\n",
      "iter 10000, loss: 176.099895\n",
      "----\n",
      " is amainted avy bed even formame, perthings lotkeviog to sha courdy ind asded oo toot arain  frer eacl and then, bloathed to genchsst of he way, thkeo Gregor was e come it hir lalk troping. Yis to tpe \n",
      "----\n",
      "iter 11000, loss: 174.001311\n",
      "----\n",
      " ve? Irvongind sim coust got to wimf (loid aller.\n",
      "\"Wous but retallo sim in's was sowe dat keat, at shiy feet lat hras \"to to hamred, hus whe raad of freibule move poed intolyad he hom. In ara just the  \n",
      "----\n",
      "iter 12000, loss: 170.488977\n",
      "----\n",
      " sibe. Ewar sheve 's beint ture for aob, \"dred himseaded to hen atroun fpely soumlone pouthing ott keroutly nowed the clear stowaid, of the. would harred that at of hit hive in that it wallen. But leck \n",
      "----\n",
      "iter 13000, loss: 166.953849\n",
      "----\n",
      "  thin frap that, say. Anf ges the derrtadd furperton. At ware\n",
      " fres they madinyto Gregor'r pany upay camen to githit  how thougs in aid into the iflest problind and and facle te tome erying. Or. INm.  \n",
      "----\n",
      "iter 14000, loss: 164.462409\n",
      "----\n",
      " to the erour\" Hiand apared hove\" sorotsads, afd the cowlimid oo the comcaro, at apsice hip.\n",
      "\n",
      "The who gice his mother simead lewsitto for of tome not was pabeer They ho her come rimean's aly omce then  \n",
      "----\n",
      "iter 15000, loss: 162.848670\n",
      "----\n",
      " ve of harss fisher, the door on but net ivenysers aid to tile him. It was inse moveredes glone been effitt her to sliaed aid to have weras wish ow sai homwisten to had prispen that. The krows. Hers'ml \n",
      "----\n",
      "iter 16000, loss: 161.282297\n",
      "----\n",
      " oll his siomer ureathone his nither turhe for bus of the concersed to get lown llouse him sadily win was lotllystibuedt, has cond of cor hreveaitef his suile tloor. Yus it on the deart aivile.\"\n",
      " Grego \n",
      "----\n",
      "iter 17000, loss: 159.989644\n",
      "----\n",
      " r of all Gregormady tho git ot nfind ig were smoridid to ken afy low lloor not his cow the fiom be ghit, his ploserd whele thaiss, his raom his nains whill gonno sick anding could sist looks wale pack \n",
      "----\n",
      "iter 18000, loss: 158.756970\n",
      "----\n",
      " s would se with a brey nover would stre the poon on ningt, whe could get ligst owen thit Gregor's mestsesnoweal ttons liok for awmstawly!\" \"I Iomo though.\n",
      "\n",
      " Iid if than his finglly ready fortimel cenl \n",
      "----\n",
      "iter 19000, loss: 156.420719\n",
      "----\n",
      " old the corthe frot what wor's, atyoced lat him cond lict to cemfacllytusis than the roog to cenle acliouse ald thiug\" Gregor antire cafearo to quimkly, \"ah ie tlo lank of his lloth aplitiduy, she say \n",
      "----\n",
      "iter 20000, loss: 153.920549\n",
      "----\n",
      "  jus warks of soums in the gagient and un fir to look to ner mo gisd courew and mores of the ragl even's beckus and at the dithse erromaspen to with his father was s inclyapee for had wough, thit's ab \n",
      "----\n",
      "iter 21000, loss: 152.885241\n",
      "----\n",
      " optlmver, beearlese to co themegs, whot the wushim, ave anayist sho dof the dwant at the miditerelyont all y wnyt the day and hasw the ilmsaytate, whele arnondy frechade\" it upper eramobes thought go  \n",
      "----\n",
      "iter 22000, loss: 151.378122\n",
      "----\n",
      " his puthant there in could now me fers hied the mouss bo sek room. Onasted he hoven, with the sure of ersained him fond of his liot? for owhy stint her ard cemery a dobed the fied ave the hiade bish l \n",
      "----\n",
      "iter 23000, loss: 150.613064\n",
      "----\n",
      " . It them tiand to has dlees ladene thought to got yousll thing herw a cime to sears, whreeve sull, some -lyond on Gregor's paint, and fres moon he, hid even oftragst a diously alazld come thas wes wa \n",
      "----\n",
      "iter 24000, loss: 150.136405\n",
      "----\n",
      "  the kidchan\" ders bowryst opllenomser to \"then Gregor!\" fart parmulabside,. I' fared in - hrags. But hat nenol, Gregor, lustime at a campla, selized \"on't the comevectiog, hiedel he was nit haad ona  \n",
      "----\n",
      "iter 25000, loss: 148.276326\n",
      "----\n",
      " into hes a pavenachowhing ofter kear netresed frol asmew yoo reasing?\n",
      "\n",
      "\"Ms were to searb it. Freacelaks. On changed to be wabth up he ffren dey wlich jo efoutly ercimed to sheat to ap a brinisld him i \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-a194a962f00f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-8bd423435043>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(inputChars, outputChars, prevH)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdb1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mdW1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#derivative of input to hidden layer weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdWr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#derivative of hidden layer to hidden layer weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mW1, mWh1, mW2 = np.zeros_like(W1), np.zeros_like(Wh1), np.zeros_like(W2)\n",
    "mb1, mb2 = np.zeros_like(b1), np.zeros_like(b2) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocabLen)*seqLength # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seqLength+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hiddenLayer,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [characterToIndex[ch] for ch in data[p:p+seqLength]]\n",
    "    targets = [characterToIndex[ch] for ch in data[p+1:p+seqLength+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dW1, dWh1, dW2, db1, db2, hprev = propagate(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([W1, Wh1, W2, b1, b2],\n",
    "    [dW1, dWh1, dW2, db1, db2],\n",
    "    [mW1, mWh1, mW2, mb1, mb2]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learningRate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seqLength # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
