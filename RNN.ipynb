{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "totalCharacters = len(data)\n",
    "vocabLen = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'p': 2, 'c': 3, 'z': 4, 'N': 5, 'x': 6, 'I': 7, '\\n': 8, '?': 9, 's': 10, 'S': 11, 'l': 12, 'F': 13, '\"': 14, 'm': 15, 'U': 16, 'k': 17, 'j': 18, 'P': 19, ',': 20, 'T': 21, 'H': 22, 'w': 23, 'y': 24, 'e': 25, 'J': 26, 'C': 27, 'L': 28, 'M': 29, '(': 30, 'r': 31, '!': 32, 'o': 33, \"'\": 34, 'q': 35, 'E': 36, 'O': 37, ' ': 38, 'n': 39, 'W': 40, '-': 41, ':': 42, 'f': 43, 'ç': 44, 'v': 45, 'V': 46, 't': 47, 'u': 48, 'G': 49, 'Y': 50, 'i': 51, 'Q': 52, 'd': 53, 'A': 54, 'D': 55, 'g': 56, 'h': 57, ')': 58, 'B': 59, '.': 60, ';': 61}\n",
      "{0: 'a', 1: 'b', 2: 'p', 3: 'c', 4: 'z', 5: 'N', 6: 'x', 7: 'I', 8: '\\n', 9: '?', 10: 's', 11: 'S', 12: 'l', 13: 'F', 14: '\"', 15: 'm', 16: 'U', 17: 'k', 18: 'j', 19: 'P', 20: ',', 21: 'T', 22: 'H', 23: 'w', 24: 'y', 25: 'e', 26: 'J', 27: 'C', 28: 'L', 29: 'M', 30: '(', 31: 'r', 32: '!', 33: 'o', 34: \"'\", 35: 'q', 36: 'E', 37: 'O', 38: ' ', 39: 'n', 40: 'W', 41: '-', 42: ':', 43: 'f', 44: 'ç', 45: 'v', 46: 'V', 47: 't', 48: 'u', 49: 'G', 50: 'Y', 51: 'i', 52: 'Q', 53: 'd', 54: 'A', 55: 'D', 56: 'g', 57: 'h', 58: ')', 59: 'B', 60: '.', 61: ';'}\n"
     ]
    }
   ],
   "source": [
    "characterToIndex = {ch:i for i,ch in enumerate(chars)}\n",
    "print(characterToIndex)\n",
    "indexToCharacter = {i:ch for i,ch in enumerate(chars)}\n",
    "print(indexToCharacter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#hyperparameters\n",
    "learningRate = 0.01\n",
    "hiddenLayer = 256\n",
    "seqLength = 50\n",
    "#modelParameters\n",
    "#connect input layer to first hidden layer\n",
    "W1 = np.random.randn(vocabLen, hiddenLayer) * 0.01\n",
    "# connect second hidden layer to output layer\n",
    "W2 = np.random.randn(hiddenLayer, vocabLen) * 0.01\n",
    "# connect first hidden layer to first hidden layer in the next timestamp\n",
    "Wh1 = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# connect the first hidden layer to the second hidden layer\n",
    "Whh = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# connect the second hidden layer to the second hidden layer in the next timestamp\n",
    "Wh2 = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# bias for W1\n",
    "b1 = np.random.randn(hiddenLayer, 1)\n",
    "# bias for Whh\n",
    "bh = np.random.randn(hiddenLayer, 1)\n",
    "# bias for W2\n",
    "b2 = np.random.randn(vocabLen, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the loss function would take in the input chars, the output chars and the previous hidden state\n",
    "# it outputs the hidden state, the gradients for each parameter between layers and the last hidden states\n",
    "def propagate(inputChars, outputChars, prevH, prevH1):\n",
    "    x, h, h1, y, p = {}, {}, {}, {}, {}\n",
    "    #x = the array which is a list of zeros, with just 1 at the index where input character is\n",
    "    #h = values of hidden layers at different times\n",
    "    #y = values of outputs not activated\n",
    "    #p = activated output\n",
    "    h[-1] = np.copy(prevH)\n",
    "    h1[-1] = np.copy(prevH1)\n",
    "    loss = 0\n",
    "    #forward propagation\n",
    "    for t in range(len(inputChars)):\n",
    "        x[t] = np.zeros((vocabLen, 1))\n",
    "        x[t][inputChars[t]] = 1\n",
    "        h[t] = np.tanh(W1.T.dot(x[t]) + Wh1.dot(h[t-1]) + b1) # h has to be of the same dimension as b\n",
    "        h1[t] = np.tanh(Whh.T.dot(h[t]) + Wh2.dot(h1[t-1]) + bh)\n",
    "        y[t] = W2.T.dot(h1[t]) + b2\n",
    "        p[t] = np.exp(y[t])/np.sum(np.exp(y[t]))\n",
    "        loss += -np.log(p[t][outputChars[t],0])\n",
    "    \n",
    "    \n",
    "    #backward propagation\n",
    "    dW1, dW2, dWh1, dWhh, dWh2 = np.zeros_like(W1), np.zeros_like(W2), np.zeros_like(Wh1), np.zeros_like(Whh), np.zeros_like(Wh2)\n",
    "    db1, db2, dbh = np.zeros_like(b1), np.zeros_like(b2), np.zeros_like(bh)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    dh1next = np.zeros_like(h1[0])\n",
    "    for t in reversed(range(len(inputChars))):\n",
    "        dy = np.copy(p[t])\n",
    "        #starting the backpropagation\n",
    "        dy[outputChars[t]] -= 1\n",
    "        \n",
    "        dW2 += h[t].dot(dy.T)\n",
    "        db2 += dy\n",
    "        \n",
    "        dh1 = np.dot(W2, dy) + dh1next\n",
    "        dh1raw = (1 - h1[t] * h1[t]) * dh1\n",
    "        dbh += dh1raw\n",
    "        dWhh += np.dot(h[t], dh1raw.T)\n",
    "        dWh2 += np.dot(dh1raw, h1[t-1].T)\n",
    "        \n",
    "        dh = np.dot(Whh, dh1) + dh1next\n",
    "        dhraw = (1 - h[t] * h[t]) * dh\n",
    "        db1 += dhraw\n",
    "        dW1 += np.dot(x[t], dhraw.T) #derivative of input to hidden layer weight\n",
    "        dWh1 += np.dot(dhraw, h[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Wh1.T, dhraw)\n",
    "    for dparam in [dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2, h[len(inputChars)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (256,256) and (128,1) not aligned: 256 (dim 1) != 128 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-b85a1cea4a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mh1prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddenLayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset RNN memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#predict the 200 next characters given 'a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh1prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcharacterToIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-b85a1cea4a63>\u001b[0m in \u001b[0;36msample\u001b[0;34m(h, h1, seed, n)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutputChars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#compute output (unnormalised)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (256,256) and (128,1) not aligned: 256 (dim 1) != 128 (dim 0)"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, h1, seed, n):\n",
    "                                                                                                                                                                                        \n",
    "    #sample a sequence of integers from the model                                                                                                                                                \n",
    "    #h is memory state, seed is seed letter for first time step   \n",
    "    #n is how many characters to predict\n",
    "\n",
    "    x = np.zeros((vocabLen, 1))\n",
    "    x[seed] = 1\n",
    "    #list to store generated chars\n",
    "    outputChars = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(W1.T, x) + np.dot(Wh1, h) + b1)\n",
    "        h1 = np.tanh(np.dot(Whh.T, h) + np.dot(Wh2, h1) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(W2.T, h1) + b2\n",
    "        # probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #print(p)\n",
    "        #pick one with the highest probability \n",
    "        selectedChar = np.random.choice(range(vocabLen), p=p.ravel())\n",
    "        #print(ix)\n",
    "        #create a vector\n",
    "        x = np.zeros((vocabLen, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[selectedChar] = 1\n",
    "        #add it to the list\n",
    "        outputChars.append(selectedChar)\n",
    "\n",
    "    txt = ''.join(indexToCharacter[char] for char in outputChars)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "    hprev = np.zeros((hiddenLayer,1)) # reset RNN memory  \n",
    "    h1prev = np.zeros((hiddenLayer,1)) # reset RNN memory \n",
    "    #predict the 200 next characters given 'a'\n",
    "sample(hprev,h1prev,characterToIndex['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 206.374703\n",
      "----\n",
      " IyYçL'YNErLsW,lJWG;D YkYL'a,BYYCYBsWqE wYokNYYYkIHlCM-m,joN.P,jY;,C:,YJMTIM.YjogWçNqwW)IVYsErfhlIIBNYBqlYIçUCTCkkIpr-ng'sT!IVEç, GBGUG,C,EMYGs.?kTNbhAUVqIA;w'YYIaYgTWçLYPI-wYVqJ',ShwQxqfIBU;uNU.wtTYI, \n",
      "----\n",
      "iter 1000, loss: 160.605083\n",
      "----\n",
      " yy act a.ade sothef çodes sot ahheasegarothle ,e soacart anr?,,uwesn kos wfs llhe hod wlediGroanet, tregaleterast gathe walchem daiSs lxr ruchens afuf caodiolereaabes noes lf unrloyaatputiink taawhe w \n",
      "----\n",
      "iter 2000, loss: 131.312170\n",
      "----\n",
      "  e thif thyuwan winpithewe beos tu himom heum'gimnre tided as wochem an tor fomichttd hele then bemey the or at a'le th wh woindrodich wuwheme ivoak waWceveaFd to. fit toad or'be hom. watleai theneod  \n",
      "----\n",
      "iter 3000, loss: 119.790208\n",
      "----\n",
      " lytet jrol peat. haluilf thet. andomm de lud ofiag mrlat oly in or shicod an ander the the sinnrabestas thes, it thid enaogind enserlilte, the at to soy fa elemeSe eoa dod cothit omeus the fot fore be \n",
      "----\n",
      "iter 4000, loss: 110.581037\n",
      "----\n",
      " e buted boon in he wabetf, angispy to apiley Somoy pod wud sisher spfolle At and woun ebale a thed to we cos sadls to hae jaog at oveleaag, Inse on is her ch ts al len to kere ther taad a thif't to cs \n",
      "----\n",
      "iter 5000, loss: 106.814686\n",
      "----\n",
      " le. fTwo thos sidps timey rewaen erolce to bime ho cedy i udatwive Madpek, the. and dide to ce mith, than. Ne be hit in ent thugh wr hen le s, wamered, were canl to ss drun he dooghe corand becte to w \n",
      "----\n",
      "iter 6000, loss: 103.041951\n",
      "----\n",
      " ge gho hay agy, hey or uld a to lay mremteing ad hiin bfet This whean; to the s mie I the could Hot her do; hoonher yNer ite thithle senderobe? has peve stolering t, ou lathalver to the. Smay as shand \n",
      "----\n",
      "iter 7000, loss: 100.348321\n",
      "----\n",
      " eomens sindre hefire ound the the shed diny rett swe fofyaing day frat, broGd samenen nabradicload theim sen and. Satde .ror in whe tinder boued hicc he andre, facf moth his couning hhiagl had siscpic \n",
      "----\n",
      "iter 8000, loss: 99.595652\n",
      "----\n",
      " lf thly the theubly on. Sunn wheofed reem her at thas aloagand he shoughs bulent had le of this thang mimn pactay fould alily vecr: elrorren \"We rr ther had aslo nomisnd is and on cely fo Gregt. \"Thir \n",
      "----\n",
      "iter 9000, loss: 96.653992\n",
      "----\n",
      " d in fasomeerd and onhed fanp toabl. That hid meong in thfur fash and of yok sely sooth boocroce astas abeny have aofulescong has shep and the hooch haclsing in his bethtiot; \"wwem as asp stilery mact \n",
      "----\n",
      "iter 10000, loss: 97.605150\n",
      "----\n",
      " er tid could a futhimr; dig, uslim pistertemow la hith he rat dring herd to it shaigh hith frelly the core, to dot deon ko seeieven? He hads, his fall to kementt in qustw hack't ostins thit; wad hain  \n",
      "----\n",
      "iter 11000, loss: 94.857461\n",
      "----\n",
      " id bouks attayt; had fot. Nouched wafle the roohikly an thas thon na prer, onsessentser, woors aly cleww alowcinquly rouk thrse fame winpsingt ob somlaans ar hedent. That in uuvave thoud would wald co \n",
      "----\n",
      "iter 12000, loss: 95.044507\n",
      "----\n",
      " -ther ow llow as his hir, Faiseding from. Pongeom bainothen, mircther.d . The\" themt of ebontsely the sod and sas in in ollers as nolk, Gpngor'cet ar kis thapl, linkive bece mafomar the minyed mamting \n",
      "----\n",
      "iter 13000, loss: 93.592759\n",
      "----\n",
      " en shick, hhall be, theyed to dee nothos of the deanint las the quemas mather move of co em'dimley ous of the usorming, yood his sigh . The roor whe cak bore oll has eregob. We wrow envy not it wise k \n",
      "----\n",
      "iter 14000, loss: 91.999240\n",
      "----\n",
      "  no- be plemr, bu, sok basarly strayory. Ft dar litg the cangitf't repbed besussed a?ln his sisterin, atrwen Jiple note talifanly beiggat at the usy the- at ouch of enfarcerp nocf and ret and pideinf  \n",
      "----\n",
      "iter 15000, loss: 92.765766\n",
      "----\n",
      " ipmting a mros leop be baths the one thae foop a lesrrew It the betning. He ands his brene pneing and bed, he seren to the asughing as evaared he prewemen, hive the blither aluted \"fee mongaly him tou \n",
      "----\n",
      "iter 16000, loss: 90.511498\n",
      "----\n",
      " bayilly mas has noob to thaalst\" Gregor wall fmow ther not whe - tardilly simfing and sllriaclery to keln and. Gre rera; the faml arpogt as dowride apvewt, murl ene te desw Gret\n",
      "r's sould fuatingel at \n",
      "----\n",
      "iter 17000, loss: 91.780129\n",
      "----\n",
      " st wicf den srael cust it ars ve hert.\n",
      "\n",
      "\n",
      "we chmallo, be kor. Alry of itw: alirgor innf. The besing dorite move bayseven fe cundcerririwester a racet; perind hour mot now to som, pim the cave thtastmus \n",
      "----\n",
      "iter 18000, loss: 89.492530\n",
      "----\n",
      " rbly tuimune seousent indore fuaded his limysaadele and dor ibar out and fuabet, it wat him the pinid shat had ward. Gregor one ofxt memy alrvyest not Dreint in Thone und for in her for to soens reall \n",
      "----\n",
      "iter 19000, loss: 89.486990\n",
      "----\n",
      " e dourd on hiw home t. Serrined it umreir. Spend fectman he doo gishor to heo\". Depr inbe yod inth th, waswind to the sheled theyed to sastersint if h stims. Was sore to dosimed on they at ever had'in \n",
      "----\n",
      "iter 20000, loss: 89.204539\n",
      "----\n",
      " and anmonide, forebigl. Thene leoand roughed hicfingy mive at the roor here teall and rowk overers nofeia, and had diraithed. Whe das fiil troorh foaming notable, eve dirgerne then of the he, on M ald \n",
      "----\n",
      "iter 21000, loss: 87.582338\n",
      "----\n",
      " llome in at the hoor sid has pome, pofgsste, apdey, herdres. At othed nooun the daars nwe ford it lathar tool. Thas he tas soor her and the deand, becomen, beent whace interver the merm.\n",
      "\n",
      "Be thin. He  \n",
      "----\n",
      "iter 22000, loss: 89.228006\n",
      "----\n",
      "  room it wremwels frert ntit themide, wandnho ho cooke loflssy of pome therally the yoor. Preblestasesien was tume in'ts, hnaped beans yed and for thiuk that Gregad qutsand the dngons he could hevenin \n",
      "----\n",
      "iter 23000, loss: 87.138452\n",
      "----\n",
      "  onn eventild relyesf nothised almusay eather caepen and repk beronorowt wastllimes theraosted toucsto, thaf the dentnow wwake uf, Gregor wimiegongt fongerstent, se mebe his bork.\n",
      "\n",
      "Ancsing hersed ibne \n",
      "----\n",
      "iter 24000, loss: 87.728553\n",
      "----\n",
      " e whill was the warsice; fram dacpre. The kegpn abongine the gethe in the say; and wasplaely to cet atsortwarkoctay on the wamso to trome thtperplen hings open\" Thero Gregurmed there albnaither, now h \n",
      "----\n",
      "iter 25000, loss: 86.289256\n",
      "----\n",
      "  and it on\"sals arpost:d it bede has nigwt bo gormor sham ats it the for stowe beely deve mofeneversess, we thaed wealally, him gonted begtre whene a was morefat\", asld cat his waving everbly'mully st \n",
      "----\n",
      "iter 26000, loss: 85.826620\n",
      "----\n",
      " , krear learifed encim a roupser the wrathy, a mand, muisd ause with the daden. St the fursoo klore wheccascerssem, shas ul hor tomeand sa toman the mighay saminicef talnly pully und reamed ather warp \n",
      "----\n",
      "iter 27000, loss: 86.371550\n",
      "----\n",
      " sith hersenge; gould lathiving loithelf to surter forg to he rer sas ate dather.\n",
      "\", s hondeld cr the asmed his r: the paything, There into of there him; buhe purnowe, but on sere and in herpengeria ch \n",
      "----\n",
      "iter 28000, loss: 84.685969\n",
      "----\n",
      " ather was of it; anwhin the cmrouckearad piclobnay hif hid the wacnuly shoir abiges oraskes thit fape pele, tasen in the kejat - to thas fict: asoards ine thewe pnound worll, in mpaired becomse the ke \n",
      "----\n",
      "iter 29000, loss: 86.253618\n",
      "----\n",
      " ote fover hound sime had be lade to haves to him alsy dave coukses be one best an itle congher shoo\", but a Grogo, \"'souad begome the burt to oxitbo ceraa ho lead mothadd, I ollit leove yow threethigh \n",
      "----\n",
      "iter 30000, loss: 84.421179\n",
      "----\n",
      "  It ppext any do coulsnds cher atreasnd and prestien rost to her bland mith blong, say alliee ont. Whot were to saaned with him her by.\n",
      "\n",
      "Sar had not onagong, and ut ser his fathen bosed aid. He ridpsa \n",
      "----\n",
      "iter 31000, loss: 85.364150\n",
      "----\n",
      "  toowh and s wast - siffaytir hinceld; as head bace mead unf evos disin his fach sevot a strapsed unfout that Gr; furtalint all wnote was wasens and \"? beem ftenged frasengy lowg hally had hand ed jus \n",
      "----\n",
      "iter 32000, loss: 84.183984\n",
      "----\n",
      " er the bod. Ast beeming ever us he ifily so lithtas ittare harsole, bube on than it slous pron tore beeaed and lept on thing the wtuin hus digher elr, it whe was store ather for have though half him a \n",
      "----\n",
      "iter 33000, loss: 83.265049\n",
      "----\n",
      " er day hat awled of wosn whoir bedy asels..\n",
      "\n",
      "Sook to lroked bun lowned thithing he havy others fxomoor. Gregor's unings that thit selp clamting her thle to he dadr as his beanotion! jaed and frouger,  \n",
      "----\n",
      "iter 34000, loss: 84.224272\n",
      "----\n",
      " opln lowled, to farling his ole ollay thalled, and his jut hand - astein not \"I shiul sen onf he tongop nock an in the cail a leat for his fowmreverant, wole warl a waive of her have could itooot, in  \n",
      "----\n",
      "iter 35000, loss: 82.642845\n",
      "----\n",
      " ng, as pfaind roabl, st. Buste to lould opced aboom; and begar at, that she wollp and mempatiot, keer and ren sol teaktcomifot and beeor. Hareng. Mpise his firted having dieased cursit,, he was cfated \n",
      "----\n",
      "iter 36000, loss: 84.297846\n",
      "----\n",
      " y rugh a soren howithrey, lucouckuted to boy ter. I fonger quing so ss fur to there tuln anowely the sheir be mo they had fust. AI hend l ougprihir he wapshought faksed to had would of the say couping \n",
      "----\n",
      "iter 37000, loss: 82.254894\n",
      "----\n",
      "  ceuthe bighes heve time to liatilebrd owador, she he coulde, wheeg; hor not erput wrorter fire pprewne be't\"'t esrentem his cat on only lighers werser. He was cafse the were him mot noorey and it gas \n",
      "----\n",
      "iter 38000, loss: 82.925725\n",
      "----\n",
      " om aad the hhindreast, litced foved to call wit kisned, he start a kand his batten a statthigf, would tild bewn iacs ilkngwing whan's bo.\n",
      "\n",
      "They de fect thelethave - thet had while lewpre and ceusing,  \n",
      "----\n",
      "iter 39000, loss: 82.474009\n",
      "----\n",
      " agarashen, yo mild ond, and seen as in that if a linht. Gren he heiddat then comeyt, thallencurouine an rean strack to the comen wugh hed. But bofay mouse to the gants by no reet heaw ffosters of the  \n",
      "----\n",
      "iter 40000, loss: 81.360719\n",
      "----\n",
      " d ly fach of their matien to thied her lett llomed ar, but herkilly houbed pertayed tore lather, be tarsed. Thihe then se the siving it witule erthith and und to ost more reoding io merr and wees they \n",
      "----\n",
      "iter 41000, loss: 82.858326\n",
      "----\n",
      " oor, dow laar rewilisest it siecaly while. The before cloasing him tay him. And wheme, enough that he would sot in coscure in the inching her upight not ho tumewithe ent, thilesed and sleigh pbe romes \n",
      "----\n",
      "iter 42000, loss: 81.275887\n",
      "----\n",
      "  so them, ferspece worl with whas more om the fares, \"Greghun'vering tGregor nall clong as they earcor, at as entpasntsont was noars the root bicnisien eres wast as reancem, percles in s mond of her a \n",
      "----\n",
      "iter 43000, loss: 82.047087\n",
      "----\n",
      " sne the chead inside, tilling dyong to fall back and thevese.\n",
      "\n",
      "Ir were in corren hhey the stitiop., nowne ofitchy wame caoch ofn in theem mere the prag. The door es his aliffer. His lotke, stonf drom  \n",
      "----\n",
      "iter 44000, loss: 80.787857\n",
      "----\n",
      " erd Sould the was juol and the was mother oot; but snat to, lot that wame. Anpote rovel the rot: that even the mide hols laded dour and upatt be had saifnly that his sand the lad regound had hery span \n",
      "----\n",
      "iter 45000, loss: 80.638146\n",
      "----\n",
      " n they spentky, kros wiyar fothough whese pas of that, but preased aid fouth fron thepe fxomed they and thaid and cor: and brhe have onyf, wrell. I's foria wur aw he jmastad moreire at all, plowartisi \n",
      "----\n",
      "iter 46000, loss: 81.061060\n",
      "----\n",
      "  and hir alwat; \"Aered, hhe thonk tood face at unly mordiptroter fre dong had bacledy had been be fomming the kemominsed in wace he roused in some career their found acmuin fated was out wo lougheld o \n",
      "----\n",
      "iter 47000, loss: 79.737319\n",
      "----\n",
      " peving at toor and buite appeally mull. The bedveale no ise gither mesening une thile Gregors, and haly was ip. Walled, event evenysore and his daorntyer aso the couders at it, dich a rother wiand in; \n",
      "----\n",
      "iter 48000, loss: 81.528114\n",
      "----\n",
      " ne for hibling it, they sas come brcerped to said of what I for, behed, Gregor had frem in wosben looking hadred that a zoichimeth, net to, abreing in frealy maiver hus novery wo hund, and ofterr. Wou \n",
      "----\n",
      "iter 49000, loss: 79.819289\n",
      "----\n",
      " ancen he lithpandly in winntound for him noous of him abmet it the dot a surnist itswe thesered, hery Anto there seen of hus everying pale molnings the forniwure had ip ghissely Gregor. Thancin and af \n",
      "----\n",
      "iter 50000, loss: 80.663273\n",
      "----\n",
      " en little and he hed into though; Mren whes everrfuth, untent to genver mish more and ald cazrene: whichald samun it nofup, he cond, aself the stoove a qulvise, the deestemthed to have? Grego tide wom \n",
      "----\n",
      "iter 51000, loss: 79.556840\n",
      "----\n",
      " ct, and roo sere left on with the saimed botted had wat been ever the gar to ether he was been in his tryanged it see fimihicesuning rull straigh as all thes mooking at, seld even it in ererpich. It p \n",
      "----\n",
      "iter 52000, loss: 79.061545\n",
      "----\n",
      " was bwate tharks and his muther, for not beer cwerouked there sly Forhione comr his stawn of bes, on eas stall room thought an her, out arfen sso cha most the vying obe, alver covy lecause it he jut a \n",
      "----\n",
      "iter 53000, loss: 79.743336\n",
      "----\n",
      " o Grero-'t thither onchard som, sur his foodeding and been ot simes of te neess, he was over thenen doppaately at firce to thing room, a it tas it then. He could in tI dit temomelly to ce, said hive o \n",
      "----\n",
      "iter 54000, loss: 78.626224\n",
      "----\n",
      " tte him to bedse, whule be owht hid moce of her som, us the greaits thack, \"haig; ssoonges, sas the juthed be har bef to thais forming for it, the lod more eanith. \"A's rather ay that heilly for as ef \n",
      "----\n",
      "iter 55000, loss: 80.226452\n",
      "----\n",
      " spreat he Inassw.\n",
      "\n",
      "Whes home apreavise hound he couce a cheawly peesed a\" was nould his sisker chat ceiring, as oriwgress, Gregor mesany to ghe chraG's it. They, lost, and showering live at habe as li \n",
      "----\n",
      "iter 56000, loss: 78.193215\n",
      "----\n",
      " easbles, not so ce thing to cliwa, alour;\", flon hald enthing alnestunaly montt anxting ouf rat passed sursating, souss douncly put to mepsthinC up hersalasly apand at teze thing de unly up acsute the \n",
      "----\n",
      "iter 57000, loss: 79.342175\n",
      "----\n",
      " ce expeprows it insul. Gregor head \"Thand streasint to toke a frened they, butoward the whoughan, wither the wourd fufpresen begaly fop the parsat for yet indo that -ou dos, le. I wallinf itmecl!\", be \n",
      "----\n",
      "iter 58000, loss: 78.628062\n",
      "----\n",
      "  I has fat could timed cent fuld at he pirent begatle; fathersef to stuemelss of mate to him pery your Gregor had not - jut to avengs. No\"'s macellong. Mringo at wlacked to with pe his ulain aon. Forn \n",
      "----\n",
      "iter 59000, loss: 77.847363\n",
      "----\n",
      " he would slese to qustint. Onoweverturesh. Df hid not too, pasinite, the wheroraindd for'ce the carsedfortan, Gregor itsly, fet ast on sifer was ouinttast a\", the thhened the door you matceropune fist \n",
      "----\n",
      "iter 60000, loss: 79.025962\n",
      "----\n",
      " ot tettlim tother thmought bany . Hingomto hissly psle of his fathor, acmird end and stuld sonnerstonp evorbly simwny eive a ala- at enterikily upeny parsiklary this geving, he somoring falp with of t \n",
      "----\n",
      "iter 61000, loss: 77.705433\n",
      "----\n",
      " er simmedpaired of dor oont his lithtent about the door any stiaadly mally duching of buct to the porgaabled fakily juite helcier him toon natel, bedy was cleak papl lut he way hartise on if selit mow \n",
      "----\n",
      "iter 62000, loss: 78.531094\n",
      "----\n",
      " whay it he chalaly fok a lled the shatks his heard wave hims thas? The onen's shith te the lell. Shere was yoom dowll, and trears mate even bef, on a drow upunat ating, buind s noving he espr it lay a \n",
      "----\n",
      "iter 63000, loss: 77.284958\n",
      "----\n",
      " es room the caml stare hlawed hamented eathed he acwo, would to fumsly in heir her up a, whening dorsher mord, with a bocyed but but he Gregor with as in fwan.\n",
      "\n",
      "I tnask and no kcoft his fasted, and up \n",
      "----\n",
      "iter 64000, loss: 77.477507\n",
      "----\n",
      " all they dad has even opken.\n",
      "\"Af the day rextrellent inl. Se that the fron shat thee? Thoa did. syes ward over take, we woud the wake his mo. They it a tones, said dayess come it had head for tenterte \n",
      "----\n",
      "iter 65000, loss: 77.712888\n",
      "----\n",
      "  tonesed, nead. \"Gr thair becemen hay emon thome alwhiet thinght boray gord if se wibsedp charleing his quld you'll te cloke so ofuns ly still porese and totely the fatcomen they woud ouply dy wroger  \n",
      "----\n",
      "iter 66000, loss: 76.681724\n",
      "----\n",
      " sen. Mreathat had - he clote their, he move wilks all she torsound backy on what they in budein, say have it\n",
      " Gregor rasandy reapilia, way ferzetturdifupty the dead ifneaque to shawevine of Gregor wac \n",
      "----\n",
      "iter 67000, loss: 78.437671\n",
      "----\n",
      " of ther by could not. He flo tiding mor.\n",
      "\"Thas oplifained thap ond op thery weing a pack eat of his farted frot the for'cent effoat towan deiston wimwerstill alloom, the buck of etnop of the diand siu \n",
      "----\n",
      "iter 68000, loss: 76.743433\n",
      "----\n",
      " sanf him thene Gregor as negangs. An his not. buspnet prom ather to dingt by that was it he dad not Gregor!f boons in the saind to steven; of his bother or, as he would comenthing agare mong, did arwa \n",
      "----\n",
      "iter 69000, loss: 77.585123\n",
      "----\n",
      " utto chuifsed a poppasiomsear, and Grcenter, bucknedbst and waze his voinded bustlet becomer andned? Thep alk again be. Thes goclisicing it the quire to by casny so hea goul quat that abret. Hein Greg \n",
      "----\n",
      "iter 70000, loss: 76.525850\n",
      "----\n",
      " aghstoustor went no pr.\n",
      "\n",
      "Sos them, not have isle hay bexinad - -ible tone oust on beer the licesthing. No had on thus his opler into therished adorg, futhing at if us the porcerentround fr-wenghmard n \n",
      "----\n",
      "iter 71000, loss: 76.125949\n",
      "----\n",
      " uth his eabled thepuderer up he haddly did fornsould in that the simsase, end wos worked bist him, trom the, on, her day, \"!ver!wand has maqming he dider there waild hen dopmed, while her back amped t \n",
      "----\n",
      "iter 72000, loss: 76.793776\n",
      "----\n",
      " l lofe to but there and gouch. Gretor prom that set to that into Gregor eveveralle as your a tnomlion?. Thicg. He hik tood pise of onot on his not be nester? But he pive to cereat's mother came the cr \n",
      "----\n",
      "iter 73000, loss: 75.701860\n",
      "----\n",
      " eded, mesh with a prost agouch, the ditthessioncommunt nowever it was there then had, the thied the firno they had doibk his pot us would negep there way futher as facherwows whe deing roog, as pe pic \n",
      "----\n",
      "iter 74000, loss: 77.315300\n",
      "----\n",
      " f thi lingly and while long, bogesled wroft buck to do west in intwangchack wound et the thongs, now hat been of all pame was tout's stild sent of mase the othistwess of his bonce at ablectomsepuing t \n",
      "----\n",
      "iter 75000, loss: 75.577032\n",
      "----\n",
      " okbred here; he had packed himself hame had the gordig, they. Somethangu tas rou paim, oave wood juastores lailes athuff where Gregor's nedeusars of the dourde apout nown in all the daceftout grat he  \n",
      "----\n",
      "iter 76000, loss: 76.817835\n",
      "----\n",
      " d einelly the rome to sallly tirunhalt to lindy had carn wast if whtardk. Anecs. That has sossebaly, saadned frobedcoom.\n",
      "\n",
      "what iatce aimion, without was sichten as an all. Of whead had toulevegund bes \n",
      "----\n",
      "iter 77000, loss: 76.072974\n",
      "----\n",
      " e than ip his hor; sears of he rown conn or the mure to closer and the mad not out hea way ontuawied in to te they wifat dorend lige at mold caming the clever ammed hit become, ho props ele leersed e! \n",
      "----\n",
      "iter 78000, loss: 75.306551\n",
      "----\n",
      " re out of ceroodile no moarc aptubly, und sus but ontent out a windored horeend, at as -oor onains all his monned ims sed levind hid tublow ronged abost hay ople hearer, mentled horan side at Gregor w \n",
      "----\n",
      "iter 79000, loss: 76.220407\n",
      "----\n",
      " and leing him the wonl the wailf then thres dmaming, with of Gregor. Has rught tupe, , was the pid ainnounated in Gregor's wamely. Gregor har at into bet. His roway nound, and chus and but of to histe \n",
      "----\n",
      "iter 80000, loss: 75.031860\n",
      "----\n",
      " hele he would net workst qut that wheer out every without was fppearely along razentry would at oncimulised as from aroshed was tent th t kes monered in his eple to him. Dut he prease, noteb simsly; e \n",
      "----\n",
      "iter 81000, loss: 76.212013\n",
      "----\n",
      " is lecamest frop an! The lin ly dight she noom for his beoring ulreasin, layt sis stepselfs\", assous tuhapbed to intied tiuch, ancaned ffout he couhevint thin, tlawling this surnthill to sinds to bein \n",
      "----\n",
      "iter 82000, loss: 74.752788\n",
      "----\n",
      "  or sorned soon agting ming enfur surpy so seved to she sithened notire her wure must he could not; she way ullefse, they his hever and but grings wiund rooks and mollsoung it the wicnked mound forst  \n",
      "----\n",
      "iter 83000, loss: 75.270560\n",
      "----\n",
      " ot - tarrnofrarttasiunly that any oxing dound get west om the testert, an-w rext stocled to she undee heen become oxere. Ane. They to the rawh to mo mat, the otched feriog and ussely, way in mouch of  \n",
      "----\n",
      "iter 84000, loss: 75.275138\n",
      "----\n",
      " uly, she whoo \"Mwer hisger\", astene thin shit I harrsiom. And then had the ce'ser shish plen as befpren of yoman dericing any leen a with helling dow cakevene, nou, one women than she back everythin?  \n",
      "----\n",
      "iter 85000, loss: 74.310178\n",
      "----\n",
      " ey had not ofcermows, lige clatish it was now rack turned the bed apenh, ald lay in delly fist s frearh the didn, fusticulow.\n",
      "\n",
      "Hatciml, they would him byforoung on the reapened it was hurder, prings \" \n",
      "----\n",
      "iter 86000, loss: 75.856420\n",
      "----\n",
      "  the uta gen every of the dister whialeven sead quite doarct's father to sald thiuges itt that Gregor was opor wotto \"spees I oned theps om if ivereately.\n",
      "\n",
      "He alat props seed him in sumed in Gregor wa \n",
      "----\n",
      "iter 87000, loss: 74.458997\n",
      "----\n",
      " nt the thom tot dayplicu away. II I the some hapis, she would beone - plady tanged some he had his sister his foning on gothing aven lew\" Hike they had come seered to kor, care to neard. Wimh before h \n",
      "----\n",
      "iter 88000, loss: 75.252136\n",
      "----\n",
      " one of the genthe bleatrerround toou him noked and ching murefunn mack ular, and sslecco on tele as thankecer of yor und woom. Waam centlese his sead how she dad the same to musten ling rusten opents. \n",
      "----\n",
      "iter 89000, loss: 74.171686\n",
      "----\n",
      "  to prear again the legping while Gretely beforeving then it clearly mightone tidned himse? Wees, al. The had bedade his potrenssy to letsher ext erorotiats. They she chess good have about airse thouf \n",
      "----\n",
      "iter 90000, loss: 74.037746\n",
      "----\n",
      " d Gregor anco still, amteed anioffomanco Ilrowly buqugeen, Greger's rait with roor.\n",
      "\n",
      "IGrsoonapen her. Theve. Lid almost on there and manwl. The regt slived stret qust rithened arwaytans she plaire, he \n",
      "----\n",
      "iter 91000, loss: 74.700233\n",
      "----\n",
      " d, and grot rise out ladyind on them? Wout now body be afthilled not brreoder of t his quined in that he had pleashed hor baren. Af conred, chene inned his fother crate makenger. Mlawer sedergass. Alt \n",
      "----\n",
      "iter 92000, loss: 73.698562\n",
      "----\n",
      "  the otieve crusistain the ghis. That's mores. \"Onow'r and walled ustenttend it t.\n",
      "\n",
      "Whey had netion only than she fur, thele, he hand after ately but the eth - maly and speel in theep to te turested d \n",
      "----\n",
      "iter 93000, loss: 74.940662\n",
      "----\n",
      "  houch of the nebk to cant his his that she peyeld must on hale for there he's way assk imjung haman trking thong to \"so klock or courst to tuntite, and had paiet; unyowt the poin then staren the efor \n",
      "----\n",
      "iter 94000, loss: 73.590262\n",
      "----\n",
      " che, rakesias\", sould with her souch, se hers had bit goo fressate and firming cho door quliched sere the faming cerwronely unterced to t fod rothouch cow. Hive strend, himself wealacainke he could al \n",
      "----\n",
      "iter 95000, loss: 74.738476\n",
      "----\n",
      "  theted wan, there cainftly gladorcands, dither to drom htacled fardunel, they hompened shisstevimy in thought sides\"y.\n",
      "\n",
      "Gregor's fral his chen?\n",
      " Sange the could not with to his cuntly a sturn. Thes w \n",
      "----\n",
      "iter 96000, loss: 73.865609\n",
      "----\n",
      " ily moreer abintlouthed his wor be las. Hig, too, now tike ding il into what and prring lowg, he could bus is his sid the roor to be ser in whe flut rore. Shat, qut ear ceeree, not tomecores a pem by  \n",
      "----\n",
      "iter 97000, loss: 73.165269\n",
      "----\n",
      " . On h math arow's fatth, tived, he had reen disc ldest at he could ather Gregor, shating't siknt Gregor, Gregor's bady was show s, \"I'dess mixh not been friggh the whought she nictle qurayt loosing q \n",
      "----\n",
      "iter 98000, loss: 74.163587\n",
      "----\n",
      " there amout let on heaming, anenjus alrooust confing the slicke ie heen us lawp, \"Grea me had no solved she rextune is oon yow, ng did plerent, and onlooble jurpered of the wickly moreing was intt, we \n",
      "----\n",
      "iter 99000, loss: 73.195979\n",
      "----\n",
      " yen, a- itckadacking abpabll twate her with whrirked froncthy Gregor had sers sildels than gas no if an the lefto the dass ward. I , dy if llokel home hed the cleinely astention than win- the entss fr \n",
      "----\n",
      "iter 100000, loss: 74.562775\n",
      "----\n",
      " y wheivine whele sutere, leck! Mess from wools could mold ea-ly dust lifands. Serpurad abanby, \"houghted horen's, ce likit of the dlike in tay ofmeer to pupe, hid inyy and by clore, the groum? and to  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mW1, mWh1, mWhh, mWh2, mW2 = np.zeros_like(W1), np.zeros_like(Wh1), np.zeros_like(Whh), np.zeros_like(Wh2), np.zeros_like(W2)\n",
    "mb1, mbh, mb2 = np.zeros_like(b1), np.zeros_like(bh), np.zeros_like(b2) # memory variables for Adagrad                                                                                                                \n",
    "smoothLoss = -np.log(1.0/vocabLen)*seqLength # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seqLength+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hiddenLayer,1)) # reset RNN memory   \n",
    "        h1prev = np.zeros((hiddenLayer,1))\n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [characterToIndex[ch] for ch in data[p:p+seqLength]]\n",
    "    targets = [characterToIndex[ch] for ch in data[p+1:p+seqLength+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2, hprev = propagate(inputs, targets, hprev, h1prev)\n",
    "    smoothLoss = smoothLoss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter %d, loss: %f' % (n, smoothLoss)) # print progress\n",
    "        sample(hprev, h1prev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([W1, Wh1, Whh, Wh2, W2, b1, bh, b2],\n",
    "    [dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2],\n",
    "    [mW1, mWh1, mWhh, mWh2, mW2, mb1, mbh, mb2]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learningRate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seqLength # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
