{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a from scratch implementation of a simple RNN having 2 hidden layers\n",
    "# it reads in any text file, tries it to learn character by character and then generates sentences\n",
    "# the basic code is taken from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "data = open('kafka.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "totalCharacters = len(data)\n",
    "vocabLen = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'p': 2, 'c': 3, 'z': 4, 'N': 5, 'x': 6, 'I': 7, '\\n': 8, '?': 9, 's': 10, 'S': 11, 'l': 12, 'F': 13, '\"': 14, 'm': 15, 'U': 16, 'k': 17, 'j': 18, 'P': 19, ',': 20, 'T': 21, 'H': 22, 'w': 23, 'y': 24, 'e': 25, 'J': 26, 'C': 27, 'L': 28, 'M': 29, '(': 30, 'r': 31, '!': 32, 'o': 33, \"'\": 34, 'q': 35, 'E': 36, 'O': 37, ' ': 38, 'n': 39, 'W': 40, '-': 41, ':': 42, 'f': 43, 'รง': 44, 'v': 45, 'V': 46, 't': 47, 'u': 48, 'G': 49, 'Y': 50, 'i': 51, 'Q': 52, 'd': 53, 'A': 54, 'D': 55, 'g': 56, 'h': 57, ')': 58, 'B': 59, '.': 60, ';': 61}\n",
      "{0: 'a', 1: 'b', 2: 'p', 3: 'c', 4: 'z', 5: 'N', 6: 'x', 7: 'I', 8: '\\n', 9: '?', 10: 's', 11: 'S', 12: 'l', 13: 'F', 14: '\"', 15: 'm', 16: 'U', 17: 'k', 18: 'j', 19: 'P', 20: ',', 21: 'T', 22: 'H', 23: 'w', 24: 'y', 25: 'e', 26: 'J', 27: 'C', 28: 'L', 29: 'M', 30: '(', 31: 'r', 32: '!', 33: 'o', 34: \"'\", 35: 'q', 36: 'E', 37: 'O', 38: ' ', 39: 'n', 40: 'W', 41: '-', 42: ':', 43: 'f', 44: 'รง', 45: 'v', 46: 'V', 47: 't', 48: 'u', 49: 'G', 50: 'Y', 51: 'i', 52: 'Q', 53: 'd', 54: 'A', 55: 'D', 56: 'g', 57: 'h', 58: ')', 59: 'B', 60: '.', 61: ';'}\n"
     ]
    }
   ],
   "source": [
    "characterToIndex = {ch:i for i,ch in enumerate(chars)}\n",
    "print(characterToIndex)\n",
    "indexToCharacter = {i:ch for i,ch in enumerate(chars)}\n",
    "print(indexToCharacter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#hyperparameters\n",
    "learningRate = 0.01\n",
    "hiddenLayer = 256\n",
    "seqLength = 50\n",
    "#modelParameters\n",
    "#connect input layer to first hidden layer\n",
    "W1 = np.random.randn(vocabLen, hiddenLayer) * 0.01\n",
    "# connect second hidden layer to output layer\n",
    "W2 = np.random.randn(hiddenLayer, vocabLen) * 0.01\n",
    "# connect first hidden layer to first hidden layer in the next timestamp\n",
    "Wh1 = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# connect the first hidden layer to the second hidden layer\n",
    "Whh = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# connect the second hidden layer to the second hidden layer in the next timestamp\n",
    "Wh2 = np.random.randn(hiddenLayer, hiddenLayer) * 0.01\n",
    "# bias for W1\n",
    "b1 = np.random.randn(hiddenLayer, 1)\n",
    "# bias for Whh\n",
    "bh = np.random.randn(hiddenLayer, 1)\n",
    "# bias for W2\n",
    "b2 = np.random.randn(vocabLen, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the loss function would take in the input chars, the output chars and the previous hidden state\n",
    "# it outputs the hidden state, the gradients for each parameter between layers and the last hidden states\n",
    "def propagate(inputChars, outputChars, prevH, prevH1):\n",
    "    x, h, h1, y, p = {}, {}, {}, {}, {}\n",
    "    #x = the array which is a list of zeros, with just 1 at the index where input character is\n",
    "    #h = values of hidden layers at different times\n",
    "    #y = values of outputs not activated\n",
    "    #p = activated output\n",
    "    h[-1] = np.copy(prevH)\n",
    "    h1[-1] = np.copy(prevH1)\n",
    "    loss = 0\n",
    "    #forward propagation\n",
    "    for t in range(len(inputChars)):\n",
    "        x[t] = np.zeros((vocabLen, 1))\n",
    "        x[t][inputChars[t]] = 1\n",
    "        h[t] = np.tanh(W1.T.dot(x[t]) + Wh1.dot(h[t-1]) + b1) # h has to be of the same dimension as b\n",
    "        h1[t] = np.tanh(Whh.T.dot(h[t]) + Wh2.dot(h1[t-1]) + bh)\n",
    "        y[t] = W2.T.dot(h1[t]) + b2\n",
    "        p[t] = np.exp(y[t])/np.sum(np.exp(y[t]))\n",
    "        loss += -np.log(p[t][outputChars[t],0])\n",
    "    \n",
    "    \n",
    "    #backward propagation\n",
    "    dW1, dW2, dWh1, dWhh, dWh2 = np.zeros_like(W1), np.zeros_like(W2), np.zeros_like(Wh1), np.zeros_like(Whh), np.zeros_like(Wh2)\n",
    "    db1, db2, dbh = np.zeros_like(b1), np.zeros_like(b2), np.zeros_like(bh)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    dh1next = np.zeros_like(h1[0])\n",
    "    for t in reversed(range(len(inputChars))):\n",
    "        dy = np.copy(p[t])\n",
    "        #starting the backpropagation\n",
    "        dy[outputChars[t]] -= 1\n",
    "        \n",
    "        dW2 += h1[t].dot(dy.T)\n",
    "        db2 += dy\n",
    "        #generally the error is backpropageted by multiplying the error in the output to the input's transpose\n",
    "        #here the transport is of the error, but this is because the way i have initialized the shape of the matrices\n",
    "        #in the original implementation, it is the standard way\n",
    "        \n",
    "        #second hidden layer backpropagation\n",
    "        dh1 = np.dot(W2, dy) + dh1next\n",
    "        #we got the below line of code by differentiation of the activation function (tanh)\n",
    "        dh1raw = (1 - h1[t] * h1[t]) * dh1\n",
    "        # so basically the activation is differentiated in two steps\n",
    "        # first is the values inside the tanh function (dh1), then finally the tanh function itself,\n",
    "        # and as from the chain rule dh1 is multiplied to it\n",
    "        dbh += dh1raw\n",
    "        dWhh += np.dot(h[t], dh1raw.T)\n",
    "        dWh2 += np.dot(dh1raw, h1[t-1].T)\n",
    "        \n",
    "        #first hidden layer backpropagation\n",
    "        dh = np.dot(Whh, dh1) + dh1next\n",
    "        dhraw = (1 - h[t] * h[t]) * dh\n",
    "        db1 += dhraw\n",
    "        dW1 += np.dot(x[t], dhraw.T) #derivative of input to hidden layer weight\n",
    "        dWh1 += np.dot(dhraw, h[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Wh1.T, dhraw)\n",
    "    for dparam in [dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2, h[len(inputChars)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ny be thill; other I Gregor's Her, bother agaivisite; by teget\", as quie us if the litter him no moven dos compress, as. The ol worky, called from cown as it as even ont to the mush being was just she \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, h1, seed, n):\n",
    "                                                                                                                                                                                        \n",
    "    #sample a sequence of integers from the model                                                                                                                                                \n",
    "    #h is memory state, seed is seed letter for first time step   \n",
    "    #n is how many characters to predict\n",
    "\n",
    "    x = np.zeros((vocabLen, 1))\n",
    "    x[seed] = 1\n",
    "    #list to store generated chars\n",
    "    outputChars = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(W1.T, x) + np.dot(Wh1, h) + b1)\n",
    "        h1 = np.tanh(np.dot(Whh.T, h) + np.dot(Wh2, h1) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(W2.T, h1) + b2\n",
    "        # probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #print(p)\n",
    "        #pick one with the highest probability \n",
    "        selectedChar = np.random.choice(range(vocabLen), p=p.ravel())\n",
    "        #print(ix)\n",
    "        #create a vector\n",
    "        x = np.zeros((vocabLen, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[selectedChar] = 1\n",
    "        #add it to the list\n",
    "        outputChars.append(selectedChar)\n",
    "\n",
    "    txt = ''.join(indexToCharacter[char] for char in outputChars)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "    hprev = np.zeros((hiddenLayer,1)) # reset RNN memory  \n",
    "    h1prev = np.zeros((hiddenLayer,1)) # reset RNN memory \n",
    "    #predict the 200 next characters given 'a'\n",
    "sample(hprev,h1prev,characterToIndex['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 206.221351\n",
      "----\n",
      " n the cin hands she low, laid.\" An the sat wall thenever and mong imposs what wo the genst; said I nitto ster parwe onts baden thaik famsed himself yis traven inmordan very hampary off stinf a living  \n",
      "----\n",
      "iter 1000, loss: 126.586990\n",
      "----\n",
      " ck, not would neck the listle than that conmile agout even the sharly his bosqubanged his morentle that was awale almost all the chest very sus little sprigh mo speak mean some after have knd soonow s \n",
      "----\n",
      "iter 2000, loss: 91.616299\n",
      "----\n",
      "  she cling and apyain, himewheo who moom in frat aghas not they caverawe actning up porsed windother, raive they monwor I cimegh on the floor that Greter, ftood in a hack culca to the onytark to hid n \n",
      "----\n",
      "iter 3000, loss: 77.983128\n",
      "----\n",
      " t it. It plopent. Ward was mubinised around but be at considess without mor and to do wiff a lut inqu te have body off soithous left ond lay then? Theer towechiny, whene!\" stapen to his it wours hape, \n",
      "----\n",
      "iter 4000, loss: 70.788932\n",
      "----\n",
      " eer to explainst, her mother to came care to fore kere to be ssear arready to barwing thous lefted understoverets'mpered to pereary feel to see hard himself to see spantlet eadlife he. Some it so it w \n",
      "----\n",
      "iter 5000, loss: 68.407973\n",
      "----\n",
      " ening there was foidly a suddat have been epproy inser. Pled to tain cone half looved heice a s in wh. hall rome by forn artended comle to tell recust more, oor had been line agaid was almocoud that n \n",
      "----\n",
      "iter 6000, loss: 66.030509\n",
      "----\n",
      " the carighen. On the could and then, which with a rindor in while heared what he compled agaly fow it warning his father omen to the clear ry, their bethifued hard of should have conmiblly fullening w \n",
      "----\n",
      "iter 7000, loss: 65.142831\n",
      "----\n",
      " oom out in from him, \"I dak; Gregor and yo adreave morearous. He had lack!\"\n",
      "\n",
      "Nofthan folds the brderearen to his because whifried cleang armight oun ge no it, yore's, shat now, opened endowt the wor m \n",
      "----\n",
      "iter 8000, loss: 64.615127\n",
      "----\n",
      " .\n",
      "\n",
      "Gregor was so his back the dissiar whing in tlow, the keys his mugh oull without legs soon, expeciar etceroded are slawn, ad the same exhaurmed a fr, liddly rush s- mornish, the iden that his bread \n",
      "----\n",
      "iter 9000, loss: 63.982641\n",
      "----\n",
      " gen asked in they haven like to get intantust enterek about while Gregor's ramely baitht, thebing se thele of be themeet stretst it, onting they been immanised to be duch whith at playuted she tame wo \n",
      "----\n",
      "iter 10000, loss: 64.181715\n",
      "----\n",
      " n though now'sards the whold, waith; the was sait coindc. Now the shing upright how he was ensies and happly time. Then's could age up the lock and reculy the wallifure. Their the edes, in his fal wha \n",
      "----\n",
      "iter 11000, loss: 63.068698\n",
      "----\n",
      " out dispress, shough, first confer exceman to his creet in the wou; would the samious no pressed. When conttainfus from when their un, grom his spencomars, sakeshing in the dight detiding for sorhor:  \n",
      "----\n",
      "iter 12000, loss: 63.728337\n",
      "----\n",
      " there unee to be itcapierdly from leant vooce? The reaving sith wherion in again his father, he done his mother to to. I'd sayeled his headactieve, he woups from Mresting at hersed to searly gooking h \n",
      "----\n",
      "iter 13000, loss: 62.843619\n",
      "----\n",
      "  belinut as from and imver the ras on the cieding her been cinsemely evening latch a geel, de it on cry. Lor'sisten had to become dorn'tient down ever us busines. Then he was a mone imsed in incin his \n",
      "----\n",
      "iter 14000, loss: 62.205162\n",
      "----\n",
      " the about. Grjour, rister'ted his father heavely back and lound not mosh with no bact a tor oncoming his mother, more ome-r we sit thunged with his sister and sans on his hix they had her mother wishe \n",
      "----\n",
      "iter 15000, loss: 62.359453\n",
      "----\n",
      " e lay thak ly and calped by helled sornow reace, whathes his wourde lighted, evention I'd, her nere, but Gregor, inde, hoorway than a stwander dich her something an in dive with ento the threw than th \n",
      "----\n",
      "iter 16000, loss: 61.871447\n",
      "----\n",
      " warseen in the evening, Gregor, gervengly and eforter or frick and she was nothing - whe carrsome more had mable fin.\n",
      "\n",
      "Whis much for\n",
      " Gregor's bet intranst, he dids; he had room brove and from is is s \n",
      "----\n",
      "iter 17000, loss: 62.348829\n",
      "----\n",
      " shing. It \", even ittont was go any place - in tas itsisised soon adly anyway't be whemen? Ye wants that not inablen that iteng him to the orms unterring's as in crose been do the congly moink. Len ou \n",
      "----\n",
      "iter 18000, loss: 61.145503\n",
      "----\n",
      " erenes acry habablerssed out ago than chey had near sunatung, swayed he was im, when he goold certed to- intention on rere though, chey went gor bell and overtly flam a slive that now, but while the m \n",
      "----\n",
      "iter 19000, loss: 61.578654\n",
      "----\n",
      " ad had, his chans's now kure so the floor favister lack undurity, the doorh asker harnfly, though, that was rell?\" Gregor mut she spot without forhing for a little tou!\", they went you'the th ithel, a \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mW1, mWh1, mWhh, mWh2, mW2 = np.zeros_like(W1), np.zeros_like(Wh1), np.zeros_like(Whh), np.zeros_like(Wh2), np.zeros_like(W2)\n",
    "mb1, mbh, mb2 = np.zeros_like(b1), np.zeros_like(bh), np.zeros_like(b2) # memory variables for Adagrad                                                                                                                \n",
    "smoothLoss = -np.log(1.0/vocabLen)*seqLength # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seqLength+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hiddenLayer,1)) # reset RNN memory   \n",
    "        h1prev = np.zeros((hiddenLayer,1))\n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [characterToIndex[ch] for ch in data[p:p+seqLength]]\n",
    "    targets = [characterToIndex[ch] for ch in data[p+1:p+seqLength+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2, hprev = propagate(inputs, targets, hprev, h1prev)\n",
    "    smoothLoss = smoothLoss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter %d, loss: %f' % (n, smoothLoss)) # print progress\n",
    "        sample(hprev, h1prev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([W1, Wh1, Whh, Wh2, W2, b1, bh, b2],\n",
    "    [dW1, dWh1, dWhh, dWh2, dW2, db1, dbh, db2],\n",
    "    [mW1, mWh1, mWhh, mWh2, mW2, mb1, mbh, mb2]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learningRate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seqLength # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
